{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.ai.generativelanguage_v1beta.types import content\n",
    "\n",
    "key_gemini = os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "genai.configure(api_key=key_gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    # \"top_k\": 64,\n",
    "    # \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "    # \"response_schema\": schema,\n",
    "    # \"response_mime_type\": \"application/json\",\n",
    "}\n",
    "    \n",
    "\n",
    "llm_gemini = genai.GenerativeModel(\n",
    "    # model_name=\"gemini-1.5-flash\",\n",
    "    # model_name=\"gemini-1.5-flash-exp-0827\",\n",
    "    model_name=\"gemini-1.5-pro-002\",\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_cheatsheet = (\n",
    "    \"create a cheatsheet of the text provide in format of list with topics and subtopics.\\n\"\n",
    "    \"output in markdown code to copy.\\n\"and\n",
    "    \"the markdown code should render in a jupyter notebook. so make sure the math formulas are in latex properly to reder in a jupyter notebook\\n\\n\"\n",
    "    \"TEXT:\\n\\n\"\n",
    "    \"{TEXT}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Probabilistic Interpretation: Why Squared Error?\n",
    "When faced with a regression problem, why might linear regression, and specifically least-squares cost function J(θ)\n",
    ", be a reasonable choice? In this section, we offer a set of probabilistic assumptions, under which least-squares regression fits into place naturally.\n",
    "Let’s use the housing prices problem for this discussion. Assume that the target variable y\n",
    ", which is the true price of every house, is a linear function of the size of the house x(i)1\n",
    " and number of bedrooms x(i)2\n",
    ", plus an error term ϵ(i)\n",
    ":\n",
    "\n",
    "y(i)=θTx(i)+ϵ(i)\n",
    "where ϵ(i)\n",
    " is an error term that captures either unmodeled effects (such as features very pertinent to predicting housing price, but that we’d left out of the regression), or random noise.\n",
    "Let’s further assume that ϵ(i)\n",
    " follows a Gaussian distribution (also called as the Normal distribution) with mean μ=0\n",
    " and variance σ2\n",
    ". Formally,\n",
    "ϵ(i)∼N(0,σ2)\n",
    "This implies that the probability density of ϵ(i)\n",
    " follows a Gaussian density function as follows:\n",
    "P(ϵ(i))=12π−−√σexp(−(ϵ(i))22σ2)\n",
    "Note that unlike the bell-shaped curve we will see for locally weighted linear regression, this equation does integrate to 1.\n",
    "Another assumption we’re going to make is that the ϵ(i)\n",
    " error terms are IID, which in statistics stands for independently and identically distributed.\n",
    "This implies that the error term for a house is independent of other houses, which might not be a true assumption. For e.g., if one house is priced unusually high, the price on a different house on the same street can probably also be unusually high (owing to say, the school district associated with that area).\n",
    "However, the assumption is practical enough to get a pretty good model.\n",
    "Under these set of assumptions: (i) ϵ(i)\n",
    " follows a Gaussian distribution and, (ii) the ϵ(i)\n",
    " error terms are IID, the probability density of y(i)\n",
    " given x(i)\n",
    " and θ\n",
    " is:\n",
    "P(y(i)|x(i);θ)=12π−−√σexp(−(y(i)−θTx(i))22σ2)\n",
    "The notation “P(y(i)∣x(i);θ)”\n",
    " is read as the probability of y(i)\n",
    " given x(i)\n",
    " and parameterized by θ\n",
    ".\n",
    "This implies that θ\n",
    " is not a random variable, but is a set of parameters that parameterize the probability distribution denoted by P(y(i)∣x(i);θ)\n",
    ".\n",
    "In other words, given x and theta, what’s the probability density of a particular house’s price? It’s a Gaussian with mean μ\n",
    " given by θTx(i)\n",
    ", and variance given by σ2\n",
    ". Formally,\n",
    "y(i)|x(i);θ∼N(θTx(i),σ2)\n",
    "The notation “y(i)∣x(i);θ”\n",
    " is read as the random variable y(i)\n",
    " given x(i)\n",
    " and parameterized by θ\n",
    " is sampled from a Gaussian distribution N(θTx(i),σ2)\n",
    ".\n",
    "We’re essentially modeling the price of a house using the random variable y\n",
    ", as a Gaussian distribution with the mean μ\n",
    " as θTx(i)\n",
    " (i.e., the “true price” of the house), and then accommodating some variation around it by adding noise σ2\n",
    " to it.\n",
    "Given the input features matrix (also called the design matrix) X\n",
    ", which contains all the x(i)\n",
    "’s and θ\n",
    ", the probability distribution of y⃗ \n",
    ", which contains all the y(i)\n",
    "’s is given by P(y⃗ ∣X;θ)\n",
    ". This quantity is typically viewed a function of X\n",
    ", for a fixed value of θ\n",
    ". When we wish to explicitly view the same quantity as a function of θ\n",
    ", we instead call it the likelihood function L(⋅)\n",
    ".\n",
    "Formally, the likelikhood of the parameters L(θ)\n",
    " is defined as:\n",
    "L(θ)=L(θ;X,y⃗ )=P(y⃗ ∣X;θ)\n",
    "Because we assumed that the errors ϵ(i)\n",
    "’s are IID (and hence also the y(i)\n",
    "’s given the x(i)\n",
    "’s), the probability of all of the observations in your training set is simply equal to the product of the probabilities:\n",
    "L(θ)=P(y⃗ ∣X;θ)=∏i=1mP(y(i)∣x(i);θ)\n",
    "Plugging in the definition of P(y(i)∣x(i);θ)\n",
    " that we had earlier,\n",
    "L(θ)=∏i=1m12π−−√σexp⎛⎝−(y(i)−θTx(i))22σ2⎞⎠\n",
    "Now, given this probabilistic model relating the y(i)\n",
    "’s and the x(i)\n",
    "’s, what is a reasonable way of choosing our best guess of the parameters θ\n",
    "? The principal of maximum likelihood says that we should choose θ\n",
    " so as to make the data as high probability as possible, i.e., we should choose θ\n",
    " to maximize L(θ)\n",
    ".\n",
    "Instead of maximizing L(θ)\n",
    ", we can also maximize any strictly monotonically increasing function of L(θ)\n",
    ". In particular, the derivations will be a bit simpler if we instead maximize the log likelihood ℓ(θ)\n",
    ":\n",
    "ℓ(θ)=logL(θ)=log∏i=1m12π−−√σexp⎛⎝−(y(i)−θTx(i))22σ2⎞⎠\n",
    "Per the product rule of logarithms, the log of a product is the sum of the logs of its factors, i.e., logxy=logx+logy\n",
    ". Applying the log product rule gives us,\n",
    "ℓ(θ)=∑i=1mlog12π−−√σexp⎛⎝−(y(i)−θTx(i))22σ2⎞⎠\n",
    "Again, using the product rule of logarithms,\n",
    "ℓ(θ)=∑i=1m⎡⎣log12π−−√σ+logexp⎛⎝−(y(i)−θTx(i))22σ2⎞⎠⎤⎦\n",
    "This simplifies to,\n",
    "ℓ(θ)=mlog12π−−√σ−1σ2⋅12∑i=1m(y(i)−θTx(i))2\n",
    "One of the well-tested methods in statistics for estimating parameters is to use maximum likelihood estimation (MLE) which means you choose θ\n",
    " to maximize the likelihood L(θ)\n",
    ". So given a dataset, an obvious way to choose θ\n",
    " is to choose the value that has the highest likelihood. In other words, choose a value of θ\n",
    " that maximizes the probability of the data P(y⃗ ∣X;θ)\n",
    ".\n",
    "Recall that to simplify the derivation, rather than maximizing the likelihood capital L(θ)\n",
    ", we maximized the log likelihood ℓ(θ)\n",
    ". Since the log(⋅)\n",
    " is a strictly monotonically increasing function, the value of θ\n",
    " that maximizes the log likelihood should be the same as the value of θ\n",
    " that maximizes the likelihood.\n",
    "Thus, if you’re using MLE, what you would like to do is choose a value of θ\n",
    " that maximizes the equation for ℓ(θ)\n",
    ". But, the first term mlog12π√σ\n",
    " is just a constant, since θ\n",
    " doesn’t even appear in this term. And so, what you would like to do is choose the value of θ\n",
    " that maximizes this second term −12∑mi=1(y(i)−θTx(i))2\n",
    ". Note that since σ\n",
    " is a constant, i.e., not a function of θ\n",
    ", the term −1σ2\n",
    " can be ignored.\n",
    "Notice the minus sign in front of the second term. Maximizing the negative of a term is the same as minimizing the term. Hence, maximizing ℓ(θ)\n",
    " is equivalent to minimizing:\n",
    "12∑i=1m(y(i)−θTx(i))2\n",
    "which we recognize to be J(θ)\n",
    ", our original least-squares cost function for linear regression.\n",
    "This proof shows that choosing the value of θ\n",
    " to minimize the least squares error cost function, is equivalent to finding the MLE for the parameters θ\n",
    " under the set of assumptions that the error terms ϵ(i)\n",
    " are Gaussian and IID.\n",
    "Key takeaways\n",
    "Under the previous probabilistic assumptions on the data, least-squares regression corresponds to finding the maximum likelihood estimate of θ\n",
    ".\n",
    "This is thus only one set of assumptions under which least-squares regression can be justified as a very natural method that’s just doing maximum likelihood estimation. Note however that the probabilistic assumptions are by no means necessary for least-squares to be a perfectly good and rational procedure, and there may be other natural assumptions that can also be used to justify it.\n",
    "Note also that, in our previous discussion, our final choice of θ\n",
    " did not depend on what was σ2\n",
    ", and indeed we’d have arrived at the same result even if σ2\n",
    " were unknown. We will use this fact again later, in our discussion on the exponential family and generalized linear models.\n",
    "Rationale for Modeling Error As a Gaussian\n",
    "Most error/noise distributions can be modeled as Gaussian distributions.\n",
    "If a distribution is made up of several noise sources which are not too correlated, then we can assume it be a Gaussian using the central limit theorem from statistics.\n",
    "For the housing prices problem, the perturbations are: (i) the mood of the seller, (ii) the school district that caters to the area, (iii) the weather in the area or, (iv) access to transportation.\n",
    "Given these uncorrelated perturbation sources, adding them up to yield the error terms ϵ(i)\n",
    " yields a distribution that can be assumed to be Gaussian.\n",
    "Likelihood vs. Probability\n",
    "The likelihood of the parameters L(θ)\n",
    " is the same as the probability of the data P(y⃗ ∣X;θ)\n",
    ". However, these exists a subtle difference between the terminology here.\n",
    "P(y⃗ ∣X;θ)\n",
    " is a function of the data y⃗ ∣X\n",
    " as well as a function of the parameters θ\n",
    ", depending on which one are you viewing as fixed and which one are you viewing as a variable.\n",
    "If you view this quantity as a function of the parameters holding the data fixed, i.e., vary parameters θ\n",
    " while keeping the training data y⃗ ∣X\n",
    " fixed, we call that the likelihood of the parameters.\n",
    "If you view this quantity as a function of the data holding the parameters fixed, i.e., vary data y⃗ ∣X\n",
    " while keeping the parameters θ\n",
    " fixed, we call that probability of the data.\n",
    "Note that θ\n",
    " is a set of parameters and is not a random variable.\n",
    " \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the markdown code should render in a jupyter notebook. so make sure the math formulas are in latex properly to reder in a jupyter notebook\n",
      "\n",
      "TEXT:\n",
      "\n",
      "\n",
      "Probabilistic Interpretation: Why Squared Error?\n",
      "When faced with a regression problem, why might linear regression, and specifically least-squares cost function J(θ)\n",
      ", be a reasonable choice? In this section, we offer a set of probabilistic assumptions, under which least-squares regression fits into place naturally.\n",
      "Let’s use the housing prices problem for this discussion. Assume that the target variable y\n",
      ", which is the true price of every house, is a linear function of the size of the house x(i)1\n",
      " and number of bedrooms x(i)2\n",
      ", plus an error term ϵ(i)\n",
      ":\n",
      "\n",
      "y(i)=θTx(i)+ϵ(i)\n",
      "where ϵ(i)\n",
      " is an error term that captures either unmodeled effects (such as features very pertinent to predicting housing price, but that we’d left out of the regression), or random noise.\n",
      "Let’s further assume that ϵ(i)\n",
      " follows a Gaussian distribution (also called as the Normal distribution) with mean μ=0\n",
      " and variance σ2\n",
      ". Formally,\n",
      "ϵ(i)∼N(0,σ2)\n",
      "This implies that the probability density of ϵ(i)\n",
      " follows a Gaussian density function as follows:\n",
      "P(ϵ(i))=12π−−√σexp(−(ϵ(i))22σ2)\n",
      "Note that unlike the bell-shaped curve we will see for locally weighted linear regression, this equation does integrate to 1.\n",
      "Another assumption we’re going to make is that the ϵ(i)\n",
      " error terms are IID, which in statistics stands for independently and identically distributed.\n",
      "This implies that the error term for a house is independent of other houses, which might not be a true assumption. For e.g., if one house is priced unusually high, the price on a different house on the same street can probably also be unusually high (owing to say, the school district associated with that area).\n",
      "However, the assumption is practical enough to get a pretty good model.\n",
      "Under these set of assumptions: (i) ϵ(i)\n",
      " follows a Gaussian distribution and, (ii) the ϵ(i)\n",
      " error terms are IID, the probability density of y(i)\n",
      " given x(i)\n",
      " and θ\n",
      " is:\n",
      "P(y(i)|x(i);θ)=12π−−√σexp(−(y(i)−θTx(i))22σ2)\n",
      "The notation “P(y(i)∣x(i);θ)”\n",
      " is read as the probability of y(i)\n",
      " given x(i)\n",
      " and parameterized by θ\n",
      ".\n",
      "This implies that θ\n",
      " is not a random variable, but is a set of parameters that parameterize the probability distribution denoted by P(y(i)∣x(i);θ)\n",
      ".\n",
      "In other words, given x and theta, what’s the probability density of a particular house’s price? It’s a Gaussian with mean μ\n",
      " given by θTx(i)\n",
      ", and variance given by σ2\n",
      ". Formally,\n",
      "y(i)|x(i);θ∼N(θTx(i),σ2)\n",
      "The notation “y(i)∣x(i);θ”\n",
      " is read as the random variable y(i)\n",
      " given x(i)\n",
      " and parameterized by θ\n",
      " is sampled from a Gaussian distribution N(θTx(i),σ2)\n",
      ".\n",
      "We’re essentially modeling the price of a house using the random variable y\n",
      ", as a Gaussian distribution with the mean μ\n",
      " as θTx(i)\n",
      " (i.e., the “true price” of the house), and then accommodating some variation around it by adding noise σ2\n",
      " to it.\n",
      "Given the input features matrix (also called the design matrix) X\n",
      ", which contains all the x(i)\n",
      "’s and θ\n",
      ", the probability distribution of y⃗ \n",
      ", which contains all the y(i)\n",
      "’s is given by P(y⃗ ∣X;θ)\n",
      ". This quantity is typically viewed a function of X\n",
      ", for a fixed value of θ\n",
      ". When we wish to explicitly view the same quantity as a function of θ\n",
      ", we instead call it the likelihood function L(⋅)\n",
      ".\n",
      "Formally, the likelikhood of the parameters L(θ)\n",
      " is defined as:\n",
      "L(θ)=L(θ;X,y⃗ )=P(y⃗ ∣X;θ)\n",
      "Because we assumed that the errors ϵ(i)\n",
      "’s are IID (and hence also the y(i)\n",
      "’s given the x(i)\n",
      "’s), the probability of all of the observations in your training set is simply equal to the product of the probabilities:\n",
      "L(θ)=P(y⃗ ∣X;θ)=∏i=1mP(y(i)∣x(i);θ)\n",
      "Plugging in the definition of P(y(i)∣x(i);θ)\n",
      " that we had earlier,\n",
      "L(θ)=∏i=1m12π−−√σexp⎛⎝−(y(i)−θTx(i))22σ2⎞⎠\n",
      "Now, given this probabilistic model relating the y(i)\n",
      "’s and the x(i)\n",
      "’s, what is a reasonable way of choosing our best guess of the parameters θ\n",
      "? The principal of maximum likelihood says that we should choose θ\n",
      " so as to make the data as high probability as possible, i.e., we should choose θ\n",
      " to maximize L(θ)\n",
      ".\n",
      "Instead of maximizing L(θ)\n",
      ", we can also maximize any strictly monotonically increasing function of L(θ)\n",
      ". In particular, the derivations will be a bit simpler if we instead maximize the log likelihood ℓ(θ)\n",
      ":\n",
      "ℓ(θ)=logL(θ)=log∏i=1m12π−−√σexp⎛⎝−(y(i)−θTx(i))22σ2⎞⎠\n",
      "Per the product rule of logarithms, the log of a product is the sum of the logs of its factors, i.e., logxy=logx+logy\n",
      ". Applying the log product rule gives us,\n",
      "ℓ(θ)=∑i=1mlog12π−−√σexp⎛⎝−(y(i)−θTx(i))22σ2⎞⎠\n",
      "Again, using the product rule of logarithms,\n",
      "ℓ(θ)=∑i=1m⎡⎣log12π−−√σ+logexp⎛⎝−(y(i)−θTx(i))22σ2⎞⎠⎤⎦\n",
      "This simplifies to,\n",
      "ℓ(θ)=mlog12π−−√σ−1σ2⋅12∑i=1m(y(i)−θTx(i))2\n",
      "One of the well-tested methods in statistics for estimating parameters is to use maximum likelihood estimation (MLE) which means you choose θ\n",
      " to maximize the likelihood L(θ)\n",
      ". So given a dataset, an obvious way to choose θ\n",
      " is to choose the value that has the highest likelihood. In other words, choose a value of θ\n",
      " that maximizes the probability of the data P(y⃗ ∣X;θ)\n",
      ".\n",
      "Recall that to simplify the derivation, rather than maximizing the likelihood capital L(θ)\n",
      ", we maximized the log likelihood ℓ(θ)\n",
      ". Since the log(⋅)\n",
      " is a strictly monotonically increasing function, the value of θ\n",
      " that maximizes the log likelihood should be the same as the value of θ\n",
      " that maximizes the likelihood.\n",
      "Thus, if you’re using MLE, what you would like to do is choose a value of θ\n",
      " that maximizes the equation for ℓ(θ)\n",
      ". But, the first term mlog12π√σ\n",
      " is just a constant, since θ\n",
      " doesn’t even appear in this term. And so, what you would like to do is choose the value of θ\n",
      " that maximizes this second term −12∑mi=1(y(i)−θTx(i))2\n",
      ". Note that since σ\n",
      " is a constant, i.e., not a function of θ\n",
      ", the term −1σ2\n",
      " can be ignored.\n",
      "Notice the minus sign in front of the second term. Maximizing the negative of a term is the same as minimizing the term. Hence, maximizing ℓ(θ)\n",
      " is equivalent to minimizing:\n",
      "12∑i=1m(y(i)−θTx(i))2\n",
      "which we recognize to be J(θ)\n",
      ", our original least-squares cost function for linear regression.\n",
      "This proof shows that choosing the value of θ\n",
      " to minimize the least squares error cost function, is equivalent to finding the MLE for the parameters θ\n",
      " under the set of assumptions that the error terms ϵ(i)\n",
      " are Gaussian and IID.\n",
      "Key takeaways\n",
      "Under the previous probabilistic assumptions on the data, least-squares regression corresponds to finding the maximum likelihood estimate of θ\n",
      ".\n",
      "This is thus only one set of assumptions under which least-squares regression can be justified as a very natural method that’s just doing maximum likelihood estimation. Note however that the probabilistic assumptions are by no means necessary for least-squares to be a perfectly good and rational procedure, and there may be other natural assumptions that can also be used to justify it.\n",
      "Note also that, in our previous discussion, our final choice of θ\n",
      " did not depend on what was σ2\n",
      ", and indeed we’d have arrived at the same result even if σ2\n",
      " were unknown. We will use this fact again later, in our discussion on the exponential family and generalized linear models.\n",
      "Rationale for Modeling Error As a Gaussian\n",
      "Most error/noise distributions can be modeled as Gaussian distributions.\n",
      "If a distribution is made up of several noise sources which are not too correlated, then we can assume it be a Gaussian using the central limit theorem from statistics.\n",
      "For the housing prices problem, the perturbations are: (i) the mood of the seller, (ii) the school district that caters to the area, (iii) the weather in the area or, (iv) access to transportation.\n",
      "Given these uncorrelated perturbation sources, adding them up to yield the error terms ϵ(i)\n",
      " yields a distribution that can be assumed to be Gaussian.\n",
      "Likelihood vs. Probability\n",
      "The likelihood of the parameters L(θ)\n",
      " is the same as the probability of the data P(y⃗ ∣X;θ)\n",
      ". However, these exists a subtle difference between the terminology here.\n",
      "P(y⃗ ∣X;θ)\n",
      " is a function of the data y⃗ ∣X\n",
      " as well as a function of the parameters θ\n",
      ", depending on which one are you viewing as fixed and which one are you viewing as a variable.\n",
      "If you view this quantity as a function of the parameters holding the data fixed, i.e., vary parameters θ\n",
      " while keeping the training data y⃗ ∣X\n",
      " fixed, we call that the likelihood of the parameters.\n",
      "If you view this quantity as a function of the data holding the parameters fixed, i.e., vary data y⃗ ∣X\n",
      " while keeping the parameters θ\n",
      " fixed, we call that probability of the data.\n",
      "Note that θ\n",
      " is a set of parameters and is not a random variable.\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_cheatsheet_formatted = prompt_cheatsheet.format(\n",
    "        TEXT=text,        \n",
    "    )\n",
    "\n",
    "print( prompt_cheatsheet_formatted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"```markdown\\n## Probabilistic Interpretation: Why Squared Error?\\n\\nWhen faced with a regression problem, why might linear regression, and specifically the least-squares cost function $J(\\\\theta)$, be a reasonable choice?  We offer a set of probabilistic assumptions under which least-squares regression fits naturally.\\n\\nConsider the housing prices problem. Assume the target variable $y$, the true price of each house, is a linear function of house size $x^{(i)}_1$ and number of bedrooms $x^{(i)}_2$, plus an error term $\\\\epsilon^{(i)}$:\\n\\n$y^{(i)} = \\\\theta^T x^{(i)} + \\\\epsilon^{(i)}$\\n\\nwhere $\\\\epsilon^{(i)}$ captures unmodeled effects or random noise.\\n\\nAssume $\\\\epsilon^{(i)}$ follows a Gaussian distribution (Normal distribution) with mean $\\\\mu = 0$ and variance $\\\\sigma^2$:\\n\\n$\\\\epsilon^{(i)} \\\\sim N(0, \\\\sigma^2)$\\n\\nThe probability density of $\\\\epsilon^{(i)}$ is:\\n\\n$P(\\\\epsilon^{(i)}) = \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma} \\\\exp\\\\left(-\\\\frac{(\\\\epsilon^{(i)})^2}{2\\\\sigma^2}\\\\right)$\\n\\nAssume the $\\\\epsilon^{(i)}$ error terms are IID (independently and identically distributed).  This implies the error term for one house is independent of others, a simplification.\\n\\nUnder these assumptions, the probability density of $y^{(i)}$ given $x^{(i)}$ and $\\\\theta$ is:\\n\\n$P(y^{(i)}|x^{(i)};\\\\theta) = \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma} \\\\exp\\\\left(-\\\\frac{(y^{(i)} - \\\\theta^T x^{(i)})^2}{2\\\\sigma^2}\\\\right)$\\n\\n$P(y^{(i)}|x^{(i)};\\\\theta)$ is the probability of $y^{(i)}$ given $x^{(i)}$ and parameterized by $\\\\theta$.  $\\\\theta$ is not a random variable but a set of parameters.  Formally:\\n\\n$y^{(i)}|x^{(i)};\\\\theta \\\\sim N(\\\\theta^T x^{(i)}, \\\\sigma^2)$\\n\\nWe model the house price $y$ as a Gaussian with mean $\\\\theta^T x^{(i)}$ (the \\\"true price\\\") and variance $\\\\sigma^2$ (noise).\\n\\nGiven the design matrix $X$ (containing all $x^{(i)}$'s) and $\\\\theta$, the probability distribution of $\\\\vec{y}$ (containing all $y^{(i)}$'s) is $P(\\\\vec{y}|X;\\\\theta)$.  Viewed as a function of $\\\\theta$, it's the likelihood function $L(\\\\cdot)$:\\n\\n$L(\\\\theta) = L(\\\\theta;X,\\\\vec{y}) = P(\\\\vec{y}|X;\\\\theta)$\\n\\nAssuming IID errors:\\n\\n$L(\\\\theta) = P(\\\\vec{y}|X;\\\\theta) = \\\\prod_{i=1}^m P(y^{(i)}|x^{(i)};\\\\theta)$\\n\\nSubstituting the definition of $P(y^{(i)}|x^{(i)};\\\\theta)$:\\n\\n$L(\\\\theta) = \\\\prod_{i=1}^m \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma} \\\\exp\\\\left(-\\\\frac{(y^{(i)} - \\\\theta^T x^{(i)})^2}{2\\\\sigma^2}\\\\right)$\\n\\nMaximum likelihood estimation (MLE) suggests choosing $\\\\theta$ to maximize $L(\\\\theta)$.  Maximizing the log-likelihood $\\\\ell(\\\\theta)$ is simpler:\\n\\n$\\\\ell(\\\\theta) = \\\\log L(\\\\theta) = \\\\log \\\\prod_{i=1}^m \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma} \\\\exp\\\\left(-\\\\frac{(y^{(i)} - \\\\theta^T x^{(i)})^2}{2\\\\sigma^2}\\\\right)$\\n\\nSimplifying using logarithm rules:\\n\\n$\\\\ell(\\\\theta) = m \\\\log \\\\frac{1}{\\\\sqrt{2\\\\pi}\\\\sigma} - \\\\frac{1}{\\\\sigma^2} \\\\cdot \\\\frac{1}{2} \\\\sum_{i=1}^m (y^{(i)} - \\\\theta^T x^{(i)})^2$\\n\\nMaximizing $\\\\ell(\\\\theta)$ is equivalent to minimizing:\\n\\n$\\\\frac{1}{2} \\\\sum_{i=1}^m (y^{(i)} - \\\\theta^T x^{(i)})^2$\\n\\nwhich is $J(\\\\theta)$, the least-squares cost function.\\n\\n## Key Takeaways\\n\\n* Least-squares regression corresponds to MLE of $\\\\theta$ under the Gaussian, IID error assumptions.\\n* These assumptions aren't necessary for least-squares to be a rational procedure.\\n* The choice of $\\\\theta$ doesn't depend on $\\\\sigma^2$.\\n\\n## Rationale for Gaussian Error\\n\\n* Many noise distributions are approximately Gaussian.\\n* The central limit theorem suggests sums of independent noise sources tend towards a Gaussian distribution.\\n\\n## Likelihood vs. Probability\\n\\n* Likelihood $L(\\\\theta)$ and probability $P(\\\\vec{y}|X;\\\\theta)$ are the same quantity, but viewed as functions of different variables.\\n* Likelihood:  $\\\\theta$ varies, data fixed.\\n* Probability: Data varies, $\\\\theta$ fixed.  \\n* $\\\\theta$ is a parameter set, not a random variable. \\n```\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\"\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 2461,\n",
       "        \"candidates_token_count\": 1151,\n",
       "        \"total_token_count\": 3612\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm_gemini.generate_content(prompt_cheatsheet_formatted)\n",
    "print( response.text[12:-3] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Probabilistic Interpretation: Why Squared Error?\n",
       "\n",
       "When faced with a regression problem, why might linear regression, and specifically the least-squares cost function $J(\\theta)$, be a reasonable choice?  We offer a set of probabilistic assumptions under which least-squares regression fits naturally.\n",
       "\n",
       "Consider the housing prices problem. Assume the target variable $y$, the true price of each house, is a linear function of house size $x^{(i)}_1$ and number of bedrooms $x^{(i)}_2$, plus an error term $\\epsilon^{(i)}$:\n",
       "\n",
       "$y^{(i)} = \\theta^T x^{(i)} + \\epsilon^{(i)}$\n",
       "\n",
       "where $\\epsilon^{(i)}$ captures unmodeled effects or random noise.\n",
       "\n",
       "Assume $\\epsilon^{(i)}$ follows a Gaussian distribution (Normal distribution) with mean $\\mu = 0$ and variance $\\sigma^2$:\n",
       "\n",
       "$\\epsilon^{(i)} \\sim N(0, \\sigma^2)$\n",
       "\n",
       "The probability density of $\\epsilon^{(i)}$ is:\n",
       "\n",
       "$P(\\epsilon^{(i)}) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(\\epsilon^{(i)})^2}{2\\sigma^2}\\right)$\n",
       "\n",
       "Assume the $\\epsilon^{(i)}$ error terms are IID (independently and identically distributed).  This implies the error term for one house is independent of others, a simplification.\n",
       "\n",
       "Under these assumptions, the probability density of $y^{(i)}$ given $x^{(i)}$ and $\\theta$ is:\n",
       "\n",
       "$P(y^{(i)}|x^{(i)};\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(y^{(i)} - \\theta^T x^{(i)})^2}{2\\sigma^2}\\right)$\n",
       "\n",
       "$P(y^{(i)}|x^{(i)};\\theta)$ is the probability of $y^{(i)}$ given $x^{(i)}$ and parameterized by $\\theta$.  $\\theta$ is not a random variable but a set of parameters.  Formally:\n",
       "\n",
       "$y^{(i)}|x^{(i)};\\theta \\sim N(\\theta^T x^{(i)}, \\sigma^2)$\n",
       "\n",
       "We model the house price $y$ as a Gaussian with mean $\\theta^T x^{(i)}$ (the \"true price\") and variance $\\sigma^2$ (noise).\n",
       "\n",
       "Given the design matrix $X$ (containing all $x^{(i)}$'s) and $\\theta$, the probability distribution of $\\vec{y}$ (containing all $y^{(i)}$'s) is $P(\\vec{y}|X;\\theta)$.  Viewed as a function of $\\theta$, it's the likelihood function $L(\\cdot)$:\n",
       "\n",
       "$L(\\theta) = L(\\theta;X,\\vec{y}) = P(\\vec{y}|X;\\theta)$\n",
       "\n",
       "Assuming IID errors:\n",
       "\n",
       "$L(\\theta) = P(\\vec{y}|X;\\theta) = \\prod_{i=1}^m P(y^{(i)}|x^{(i)};\\theta)$\n",
       "\n",
       "Substituting the definition of $P(y^{(i)}|x^{(i)};\\theta)$:\n",
       "\n",
       "$L(\\theta) = \\prod_{i=1}^m \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(y^{(i)} - \\theta^T x^{(i)})^2}{2\\sigma^2}\\right)$\n",
       "\n",
       "Maximum likelihood estimation (MLE) suggests choosing $\\theta$ to maximize $L(\\theta)$.  Maximizing the log-likelihood $\\ell(\\theta)$ is simpler:\n",
       "\n",
       "$\\ell(\\theta) = \\log L(\\theta) = \\log \\prod_{i=1}^m \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(y^{(i)} - \\theta^T x^{(i)})^2}{2\\sigma^2}\\right)$\n",
       "\n",
       "Simplifying using logarithm rules:\n",
       "\n",
       "$\\ell(\\theta) = m \\log \\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{\\sigma^2} \\cdot \\frac{1}{2} \\sum_{i=1}^m (y^{(i)} - \\theta^T x^{(i)})^2$\n",
       "\n",
       "Maximizing $\\ell(\\theta)$ is equivalent to minimizing:\n",
       "\n",
       "$\\frac{1}{2} \\sum_{i=1}^m (y^{(i)} - \\theta^T x^{(i)})^2$\n",
       "\n",
       "which is $J(\\theta)$, the least-squares cost function.\n",
       "\n",
       "## Key Takeaways\n",
       "\n",
       "* Least-squares regression corresponds to MLE of $\\theta$ under the Gaussian, IID error assumptions.\n",
       "* These assumptions aren't necessary for least-squares to be a rational procedure.\n",
       "* The choice of $\\theta$ doesn't depend on $\\sigma^2$.\n",
       "\n",
       "## Rationale for Gaussian Error\n",
       "\n",
       "* Many noise distributions are approximately Gaussian.\n",
       "* The central limit theorem suggests sums of independent noise sources tend towards a Gaussian distribution.\n",
       "\n",
       "## Likelihood vs. Probability\n",
       "\n",
       "* Likelihood $L(\\theta)$ and probability $P(\\vec{y}|X;\\theta)$ are the same quantity, but viewed as functions of different variables.\n",
       "* Likelihood:  $\\theta$ varies, data fixed.\n",
       "* Probability: Data varies, $\\theta$ fixed.  \n",
       "* $\\theta$ is a parameter set, not a random variable. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "markdown_code = response.text[12:-3]\n",
    "\n",
    "display(Markdown(markdown_code))\n",
    "\n",
    "# print( markdown_code )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
