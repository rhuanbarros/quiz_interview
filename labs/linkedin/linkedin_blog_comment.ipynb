{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/quiz_interview/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from google.ai.generativelanguage_v1beta.types import content\n",
    "\n",
    "key_gemini = os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "genai.configure(api_key=key_gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "generation_config = {\n",
    "    \"temperature\": 0.75,\n",
    "    \"top_p\": 0.95,\n",
    "    # \"top_k\": 64,\n",
    "    # \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "    # \"response_schema\": schema,\n",
    "    # \"response_mime_type\": \"application/json\",\n",
    "}\n",
    "    \n",
    "\n",
    "llm_gemini = genai.GenerativeModel(\n",
    "    # model_name=\"gemini-1.5-flash\",\n",
    "    # model_name=\"gemini-1.5-flash-exp-0827\",\n",
    "    model_name=\"gemini-1.5-pro-002\",\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"I will give you a blog post from linkedin.\\n\"\n",
    "    \"You should help me create a good comment to it.\\n\"\n",
    "    \"it should have 2 setences at most\\n\"\n",
    "    # \"Maximum of {QTY_WORDS} words \\n\"\n",
    "    \"it shoud be casual like a conversation.\\n\"\n",
    "    \"you dont need to be super excited.\\n\"\n",
    "    \"generate some meaninful examples considering some generic chalanges professionals may have with the tech stack.\\n\"\n",
    "    \"Dont talking about some case that could happend to me. Dont do it talk just about generic things. Dont say that somethin had happend to me.\\n\"\n",
    "    \"dont make questions \\n\"\n",
    "    \"Create {QTY_COMMENTS} example comments\\n\"\n",
    "    \"I will give you a list of examples with the explanation of if it is a good or bad example. \\n\"\n",
    "    \"{EXAMPLES}\"    \n",
    "    \"BLOG POST:\\n\\n\"\n",
    "    \"{BLOG}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOG = \"\"\"\n",
    "\n",
    "Creating a Data Mart in Azure Fabrics: A Step-by-Step Guide\n",
    "Jean Faustino\n",
    "Jean Faustino\n",
    "Data Engineer | Azure & Python Specialist | ETL & Data Pipeline Expert\n",
    "\n",
    "\n",
    "November 14, 2024\n",
    "Creating a Data Mart in Azure Fabric: A Step-by-Step Guide\n",
    "Understanding Data Marts and Their Importance\n",
    "A data mart is a subset of an organization’s data warehouse, designed to store and manage data for specific business units or departments. This specialized approach of data engineering focuses on making relevant subsets of data readily available, thereby enhancing the decision-making capabilities of those units. Through the integration of SQL and tools provided by platforms like Microsoft Azure, organizations can create and maintain data marts that cater specifically to the analytical needs of various stakeholders.\n",
    "\n",
    "The primary function of a data mart is to enable faster and more efficient data access tailored to particular departments, such as sales, marketing, finance, or human resources. Unlike traditional data warehouses that may house a vast array of data without specific targeting, data marts are streamlined, meaning users can conduct queries and analyses with minimal navigation through irrelevant information. This targeted approach not only improves the speed of query performance but also elevates the overall effectiveness of data retrieval.\n",
    "\n",
    "One of the significant benefits of implementing a data mart is the improvement in decision-making processes. With access to targeted data sets, department heads and analysts can derive insights that are pertinent to their operations, leading to timely and informed decisions. Additionally, by utilizing Microsoft Azure technologies, organizations benefit from a scalable infrastructure that allows for cost-effective data management while ensuring security and compliance.\n",
    "\n",
    "Furthermore, data marts facilitate self-service analytics, empowering users who may not possess extensive technical skills in SQL or data engineering to engage in data exploration. This democratization of data access plays a crucial role in fostering a data-driven culture within an organization. In essence, data marts serve not only as efficient repositories of information but also as essential tools for enhancing organizational agility, particularly in making data-informed strategic decisions.\n",
    "\n",
    "Prerequisites for Building a Data Mart in Azure Fabrics\n",
    "Creating a data mart in Azure Fabric involves an understanding of various prerequisites that are essential for a successful implementation. First and foremost, proficiency in Microsoft Azure services is critical. Familiarity with the cloud platform’s architecture can significantly streamline the development process. Essential knowledge in areas such as Azure SQL Database, which is pivotal for data storage and querying, is also required. Data engineers should have hands-on experience with this service to effectively manage data retrieval and analysis.\n",
    "\n",
    "Moreover, understanding data modeling concepts is fundamental. Data engineers need to be well-versed in how to structure and organize data within a data mart. Familiarity with star schema and snowflake schema techniques can help in designing efficient databases that support analytical queries. Awareness of the relationships between dimensions and facts within the data is crucial for creating a reliable data architecture.\n",
    "\n",
    "Another key component involves the use of Azure Data Factory. This tool is essential for data integration, allowing engineers to create data pipelines that transport data from various sources into the data mart. Understanding how to utilize Azure Data Factory will enable data engineers to automate data workflows, ensuring timely data availability.\n",
    "\n",
    "Moreover, appropriate permissions and subscriptions are necessary to leverage Microsoft Azure services effectively. A valid Azure subscription grants access to the resources needed to create and manage your data mart. It's advisable for teams to ensure they possess the required role-based access controls (RBAC) associated with the Azure resources involved in the project. This combination of knowledge in Azure services, data modeling, and tool proficiency ensures that data engineers can build a robust and efficient data mart in Azure Fabrics.\n",
    "\n",
    "Step-by-Step Process to Create a Data Mart in Azure Fabric\n",
    "Creating a data mart in Azure Fabric involves a series of systematic steps, focusing on efficiency and effectiveness in data handling. The first phase of the process entails retrieving data from the source systems. It is essential to identify the relevant data sources, which could range from transactional databases to flat files and APIs. Once identified, you can use Azure Data Factory to connect to these sources. Proper connectivity ensures that data extraction is seamless and reliable.\n",
    "\n",
    "Next, the extracted data often requires transformation to meet the analytical needs of the business. This is where Azure Data Factory’s data transformation capabilities become invaluable. By utilizing SQL queries to perform necessary calculations or restructuring, you will prepare the data for its final destination. It is important to follow best practices in this step, such as maintaining data integrity and consistency. Use data validation techniques to ensure the transformed data is accurate and conforms to your predefined schema.\n",
    "\n",
    "Once the data has been transformed, the next step is to load it into the desired destination, often an Azure SQL Database. Utilizing the appropriate loading tools in Azure Data Factory helps streamline this process. During the loading phase, one must ensure that data is organized effectively for optimal query performance, especially to accommodate future reporting requirements. Pay careful attention to the indexing strategy and partitioning to enhance retrieval speeds. Additionally, consider implementing logging and monitoring to track the data loading process, allowing for timely troubleshooting of any issues that may arise.\n",
    "\n",
    "In conclusion, following these structured steps—retrieving, transforming, and loading data—while adhering to best practices will facilitate the effective creation of a robust data mart in Azure Fabric. This structured approach not only enhances data quality but also establishes a solid groundwork for future analytics endeavors.\n",
    "\n",
    "Testing and Optimizing Your Data Mart\n",
    "Once the data mart has been established in Azure Fabric, it is essential to conduct thorough testing to ensure data accuracy and performance efficiency. As a dataengineer, understanding the importance of data validation and optimization for smooth data operations becomes crucial. Testing should begin with validating the data integrity; this involves verifying that the right data has been ingested from source systems and stored correctly in the data mart. Common methods for validation include comparisons between the source and destination datasets, checksum verifications, and employing SQL queries to check for anomalies. These processes ensure that users can rely on the data they access for reporting and analytics.\n",
    "\n",
    "In addition to data validation, performance optimization plays a significant role in a well-functioning data mart. One effective strategy is to implement indexing techniques that help improve query performance. Indexing allows SQL queries to access relevant data more quickly, resulting in decreased response times during data retrieval. Dataengineers should evaluate which indexing strategy best suits their use cases—whether it’s creating clustered or non-clustered indexes or choosing to enable full-text indexing based on the data types being queried.\n",
    "\n",
    "Moreover, optimizing SQL queries can yield substantial performance improvements. This might involve simplifying complex queries, avoiding unnecessary joins, or utilizing common table expressions. Azure provides a variety of monitoring tools that can assist dataengineers in measuring the performance of their data marts. Tools such as Azure Monitor and Azure SQL Analytics offer insights into query performance, resource utilization, and workload patterns. Continuous assessment and adjustment based on these metrics ensures that the data mart remains efficient, effectively supports user needs, and adapts to evolving data demands.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLES = \"\"\"\n",
    "BAD EXAMPLES - dont generate more like this.\n",
    "\n",
    "example 1) We had a similar issue, but our bottleneck was in the message queue.  Observability was key to untangling that mess, especially with distributed tracing.\n",
    "example 1 explanation)  'We had a similar issue' dont talk like we had some case in the past.\n",
    "\n",
    "example 2) It's amazing how blind you can be without proper observability.  We were chasing ghosts for weeks until we finally got some decent metrics in place.\n",
    "example 2 explanation) 'We were chasing ghosts' dont talk like we had some case in the past\n",
    "\n",
    "example 3)The performance gains from mixed precision are compelling, but integrating it into our existing training pipeline has been more involved than we initially anticipated. There's always a learning curve with new techniques.\n",
    "example 3 explanation) 'integrating it into our existing training pipeline' dont talk like we had some case in the past\n",
    "\n",
    "example 4)Solid guide!  We're also using Azure Data Factory, and managing the pipeline complexity across different data sources can be tricky.\n",
    "example 4 explanation) 'We're also using' dont talk like we had some case in the past\n",
    "\n",
    "example 5) Data validation is crucial – we've learned the hard way that automated data quality checks save a lot of headaches down the line.\n",
    "example 5 explanation) 'we've learned' dont talk like we had some case in the past\n",
    "\n",
    "GOOD EXAMPLES - generate more like this.\n",
    "\n",
    "example 6) Sometimes the sheer volume of data from observability tools can be overwhelming.  Filtering and focusing on the right metrics is an art in itself.\n",
    "example 6 explanation) a good and generic way to talk about some generic thing,\n",
    "\n",
    "example 7) Shifting from reactive firefighting to proactive optimization is great thanks to observability, but getting buy-in for the initial investment can be tough.\n",
    "example 7 explanation)  a good and generic way to talk about some generic thing,\n",
    "\n",
    "example 8) Love how you broke down the key benefits of mixed precision training!\n",
    "example 8 explanation) good becasue it mentions how well he explaned the topic in a generic way.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will give you a blog post from linkedin.\n",
      "You should help me create a good comment to it.\n",
      "it should have 2 setences at most\n",
      "it shoud be casual like a conversation.\n",
      "you dont need to be super excited.\n",
      "generate some meaninful examples considering some generic chalanges professionals may have with the tech stack.\n",
      "Dont talking about some case that could happend to me. Dont do it talk just about generic things. Dont say that somethin had happend to me.\n",
      "dont make questions \n",
      "Create 5 example comments\n",
      "I will give you a list of examples with the explanation of if it is a good or bad example. \n",
      "\n",
      "BAD EXAMPLES - dont generate more like this.\n",
      "\n",
      "example 1) We had a similar issue, but our bottleneck was in the message queue.  Observability was key to untangling that mess, especially with distributed tracing.\n",
      "example 1 explanation)  'We had a similar issue' dont talk like we had some case in the past.\n",
      "\n",
      "example 2) It's amazing how blind you can be without proper observability.  We were chasing ghosts for weeks until we finally got some decent metrics in place.\n",
      "example 2 explanation) 'We were chasing ghosts' dont talk like we had some case in the past\n",
      "\n",
      "example 3)The performance gains from mixed precision are compelling, but integrating it into our existing training pipeline has been more involved than we initially anticipated. There's always a learning curve with new techniques.\n",
      "example 3 explanation) 'integrating it into our existing training pipeline' dont talk like we had some case in the past\n",
      "\n",
      "example 4)Solid guide!  We're also using Azure Data Factory, and managing the pipeline complexity across different data sources can be tricky.\n",
      "example 4 explanation) 'We're also using' dont talk like we had some case in the past\n",
      "\n",
      "example 5) Data validation is crucial – we've learned the hard way that automated data quality checks save a lot of headaches down the line.\n",
      "example 5 explanation) 'we've learned' dont talk like we had some case in the past\n",
      "\n",
      "GOOD EXAMPLES - generate more like this.\n",
      "\n",
      "example 6) Sometimes the sheer volume of data from observability tools can be overwhelming.  Filtering and focusing on the right metrics is an art in itself.\n",
      "example 6 explanation) a good and generic way to talk about some generic thing,\n",
      "\n",
      "example 7) Shifting from reactive firefighting to proactive optimization is great thanks to observability, but getting buy-in for the initial investment can be tough.\n",
      "example 7 explanation)  a good and generic way to talk about some generic thing,\n",
      "\n",
      "example 8) Love how you broke down the key benefits of mixed precision training!\n",
      "example 8 explanation) good becasue it mentions how well he explaned the topic in a generic way.\n",
      "\n",
      "BLOG POST:\n",
      "\n",
      "\n",
      "\n",
      "Creating a Data Mart in Azure Fabrics: A Step-by-Step Guide\n",
      "Jean Faustino\n",
      "Jean Faustino\n",
      "Data Engineer | Azure & Python Specialist | ETL & Data Pipeline Expert\n",
      "\n",
      "\n",
      "November 14, 2024\n",
      "Creating a Data Mart in Azure Fabric: A Step-by-Step Guide\n",
      "Understanding Data Marts and Their Importance\n",
      "A data mart is a subset of an organization’s data warehouse, designed to store and manage data for specific business units or departments. This specialized approach of data engineering focuses on making relevant subsets of data readily available, thereby enhancing the decision-making capabilities of those units. Through the integration of SQL and tools provided by platforms like Microsoft Azure, organizations can create and maintain data marts that cater specifically to the analytical needs of various stakeholders.\n",
      "\n",
      "The primary function of a data mart is to enable faster and more efficient data access tailored to particular departments, such as sales, marketing, finance, or human resources. Unlike traditional data warehouses that may house a vast array of data without specific targeting, data marts are streamlined, meaning users can conduct queries and analyses with minimal navigation through irrelevant information. This targeted approach not only improves the speed of query performance but also elevates the overall effectiveness of data retrieval.\n",
      "\n",
      "One of the significant benefits of implementing a data mart is the improvement in decision-making processes. With access to targeted data sets, department heads and analysts can derive insights that are pertinent to their operations, leading to timely and informed decisions. Additionally, by utilizing Microsoft Azure technologies, organizations benefit from a scalable infrastructure that allows for cost-effective data management while ensuring security and compliance.\n",
      "\n",
      "Furthermore, data marts facilitate self-service analytics, empowering users who may not possess extensive technical skills in SQL or data engineering to engage in data exploration. This democratization of data access plays a crucial role in fostering a data-driven culture within an organization. In essence, data marts serve not only as efficient repositories of information but also as essential tools for enhancing organizational agility, particularly in making data-informed strategic decisions.\n",
      "\n",
      "Prerequisites for Building a Data Mart in Azure Fabrics\n",
      "Creating a data mart in Azure Fabric involves an understanding of various prerequisites that are essential for a successful implementation. First and foremost, proficiency in Microsoft Azure services is critical. Familiarity with the cloud platform’s architecture can significantly streamline the development process. Essential knowledge in areas such as Azure SQL Database, which is pivotal for data storage and querying, is also required. Data engineers should have hands-on experience with this service to effectively manage data retrieval and analysis.\n",
      "\n",
      "Moreover, understanding data modeling concepts is fundamental. Data engineers need to be well-versed in how to structure and organize data within a data mart. Familiarity with star schema and snowflake schema techniques can help in designing efficient databases that support analytical queries. Awareness of the relationships between dimensions and facts within the data is crucial for creating a reliable data architecture.\n",
      "\n",
      "Another key component involves the use of Azure Data Factory. This tool is essential for data integration, allowing engineers to create data pipelines that transport data from various sources into the data mart. Understanding how to utilize Azure Data Factory will enable data engineers to automate data workflows, ensuring timely data availability.\n",
      "\n",
      "Moreover, appropriate permissions and subscriptions are necessary to leverage Microsoft Azure services effectively. A valid Azure subscription grants access to the resources needed to create and manage your data mart. It's advisable for teams to ensure they possess the required role-based access controls (RBAC) associated with the Azure resources involved in the project. This combination of knowledge in Azure services, data modeling, and tool proficiency ensures that data engineers can build a robust and efficient data mart in Azure Fabrics.\n",
      "\n",
      "Step-by-Step Process to Create a Data Mart in Azure Fabric\n",
      "Creating a data mart in Azure Fabric involves a series of systematic steps, focusing on efficiency and effectiveness in data handling. The first phase of the process entails retrieving data from the source systems. It is essential to identify the relevant data sources, which could range from transactional databases to flat files and APIs. Once identified, you can use Azure Data Factory to connect to these sources. Proper connectivity ensures that data extraction is seamless and reliable.\n",
      "\n",
      "Next, the extracted data often requires transformation to meet the analytical needs of the business. This is where Azure Data Factory’s data transformation capabilities become invaluable. By utilizing SQL queries to perform necessary calculations or restructuring, you will prepare the data for its final destination. It is important to follow best practices in this step, such as maintaining data integrity and consistency. Use data validation techniques to ensure the transformed data is accurate and conforms to your predefined schema.\n",
      "\n",
      "Once the data has been transformed, the next step is to load it into the desired destination, often an Azure SQL Database. Utilizing the appropriate loading tools in Azure Data Factory helps streamline this process. During the loading phase, one must ensure that data is organized effectively for optimal query performance, especially to accommodate future reporting requirements. Pay careful attention to the indexing strategy and partitioning to enhance retrieval speeds. Additionally, consider implementing logging and monitoring to track the data loading process, allowing for timely troubleshooting of any issues that may arise.\n",
      "\n",
      "In conclusion, following these structured steps—retrieving, transforming, and loading data—while adhering to best practices will facilitate the effective creation of a robust data mart in Azure Fabric. This structured approach not only enhances data quality but also establishes a solid groundwork for future analytics endeavors.\n",
      "\n",
      "Testing and Optimizing Your Data Mart\n",
      "Once the data mart has been established in Azure Fabric, it is essential to conduct thorough testing to ensure data accuracy and performance efficiency. As a dataengineer, understanding the importance of data validation and optimization for smooth data operations becomes crucial. Testing should begin with validating the data integrity; this involves verifying that the right data has been ingested from source systems and stored correctly in the data mart. Common methods for validation include comparisons between the source and destination datasets, checksum verifications, and employing SQL queries to check for anomalies. These processes ensure that users can rely on the data they access for reporting and analytics.\n",
      "\n",
      "In addition to data validation, performance optimization plays a significant role in a well-functioning data mart. One effective strategy is to implement indexing techniques that help improve query performance. Indexing allows SQL queries to access relevant data more quickly, resulting in decreased response times during data retrieval. Dataengineers should evaluate which indexing strategy best suits their use cases—whether it’s creating clustered or non-clustered indexes or choosing to enable full-text indexing based on the data types being queried.\n",
      "\n",
      "Moreover, optimizing SQL queries can yield substantial performance improvements. This might involve simplifying complex queries, avoiding unnecessary joins, or utilizing common table expressions. Azure provides a variety of monitoring tools that can assist dataengineers in measuring the performance of their data marts. Tools such as Azure Monitor and Azure SQL Analytics offer insights into query performance, resource utilization, and workload patterns. Continuous assessment and adjustment based on these metrics ensures that the data mart remains efficient, effectively supports user needs, and adapts to evolving data demands.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_formatted = prompt.format(\n",
    "        BLOG=BLOG,\n",
    "        # QTY_WORDS=30,\n",
    "        QTY_COMMENTS=5,\n",
    "        EXAMPLES=EXAMPLES\n",
    "    )\n",
    "\n",
    "print( prompt_formatted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = llm_gemini.generate_content(prompt_formatted)\n",
    "chat = llm_gemini.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  Managing permissions and access control within Azure can sometimes feel like navigating a maze.  Clearly defined roles and responsibilities are key to avoiding headaches down the road.\n",
      "\n",
      "2.  Data transformation is often the most time-consuming part of the process. Finding the right balance between ETL tool capabilities and custom scripting can be a challenge.\n",
      "\n",
      "3.  While Azure Data Factory offers a lot of flexibility, keeping the pipelines maintainable and understandable as they grow in complexity is an ongoing effort.\n",
      "\n",
      "4.  It's easy to get caught up in the technical details of building a data mart, but aligning it with actual business needs is what truly matters.\n",
      "\n",
      "5.  Ensuring data quality in the long run requires more than just initial validation.  Automated monitoring and alerting are essential for catching issues proactively.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(prompt_formatted)\n",
    "print( response.text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = chat.send_message(text)\n",
    "print( response.text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
