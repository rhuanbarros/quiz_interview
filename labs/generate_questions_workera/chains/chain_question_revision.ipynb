{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question revision chain\n",
    "\n",
    "- get a question\n",
    "- LLM A and B verify if the question is OK\n",
    "    - there is more than one option correct\n",
    "    - sugests modifications to improve the questions and options\n",
    "    - sugest a detailed explanation to give to the student\n",
    "- verify \n",
    "    - if they diverge, calls another LLM to solve vote\n",
    "    - if ok, merge using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json_repair\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 953 entries, 0 to 952\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   id                 953 non-null    int64 \n",
      " 1   created_at         953 non-null    object\n",
      " 2   subject_matter     953 non-null    object\n",
      " 3   topic_description  953 non-null    object\n",
      " 4   level              953 non-null    object\n",
      " 5   question           953 non-null    object\n",
      " 6   type               953 non-null    object\n",
      " 7   answer_correct     953 non-null    object\n",
      " 8   explanation        953 non-null    object\n",
      " 9   answer_a           953 non-null    object\n",
      " 10  answer_b           953 non-null    object\n",
      " 11  answer_c           953 non-null    object\n",
      " 12  answer_d           953 non-null    object\n",
      "dtypes: int64(1), object(12)\n",
      "memory usage: 96.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"questions.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                  11\n",
       "created_at                               2024-09-22 12:31:43.546285+00\n",
       "subject_matter                                             Probability\n",
       "topic_description                       Understanding Confusion Matrix\n",
       "level                                                     1 - Remember\n",
       "question             Which term represents correctly classified pos...\n",
       "type                                                  multiple_options\n",
       "answer_correct                                                       a\n",
       "explanation          True Positives (TP) represent the number of ca...\n",
       "answer_a                                                True Positives\n",
       "answer_b                                               False Positives\n",
       "answer_c                                               False Negatives\n",
       "answer_d                                                True Negatives\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = df.iloc[0]\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which term represents correctly classified positive cases?'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question[\"question\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.ai.generativelanguage_v1beta.types import content\n",
    "\n",
    "key_gemini = os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "genai.configure(api_key=key_gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = content.Schema(\n",
    "    type=content.Type.OBJECT,\n",
    "    properties={\n",
    "        \"option_a_explanation\": content.Schema(\n",
    "            type=content.Type.STRING,\n",
    "            description=\"Detailed explanation of why the option 'a' is correct or not.\",\n",
    "        ),\n",
    "        \"option_b_explanation\": content.Schema(\n",
    "            type=content.Type.STRING,\n",
    "            description=\"Detailed explanation of why the option 'b' is correct or not.\",\n",
    "        ),\n",
    "        \"option_c_explanation\": content.Schema(\n",
    "            type=content.Type.STRING,\n",
    "            description=\"Detailed explanation of why the option 'c' is correct or not.\",\n",
    "        ),\n",
    "        \"option_d_explanation\": content.Schema(\n",
    "            type=content.Type.STRING,\n",
    "            description=\"Detailed explanation of why the option 'd' is correct or not.\",\n",
    "        ),\n",
    "        \"just_one_option_correct\": content.Schema(\n",
    "            type=content.Type.BOOLEAN,\n",
    "            description=\"If there is just one option correct.\",\n",
    "        ),\n",
    "        \"correct_answer\": content.Schema(\n",
    "            type=content.Type.STRING,\n",
    "            description=\"The correct option. It could be 'a', 'b', 'c', 'd'.\",\n",
    "        ),\n",
    "    },\n",
    "    required=[\n",
    "        \"option_a_explanation\",\n",
    "        \"option_b_explanation\",\n",
    "        \"option_c_explanation\",\n",
    "        \"option_d_explanation\",\n",
    "        \"just_one_option_correct\",\n",
    "        \"correct_answer\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    # \"top_k\": 64,\n",
    "    # \"max_output_tokens\": 8192,\n",
    "    \"response_schema\": schema,\n",
    "    \"response_mime_type\": \"application/json\",\n",
    "}\n",
    "    \n",
    "\n",
    "llm_gemini = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=generation_config,\n",
    "    # safety_settings = Adjust safety settings\n",
    "    # See https://ai.google.dev/gemini-api/docs/safety-settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_revision = (\n",
    "    \"# BACKGROUND INFORMATION OF YOUR ROLE:\\n\"\n",
    "    \"You are an expert in machine learning.\\n\"\n",
    "    \"You are verifying if a question from a bank of question is ok to be used in a job interview.\\n\"\n",
    "    \"your task is to analyse the provided question to verify if there is some problem.\\n\"\n",
    "    \"You shoud verify if there is just one option correct or if there is more than one.\\n\"\n",
    "    \"The question could be perfect fine, so you could say it too.\\n\"\n",
    "    \"Also, you should give a detailed explanation for your answer.\\n\"\n",
    "    \"# QUESTION\\n\\n\"\n",
    "    \"{question}\\n\\n\"\n",
    "    \"# Option a\\n\"\n",
    "    \"{option_a}\\n\\n\"\n",
    "    \"# Option b\\n\"\n",
    "    \"{option_b}\\n\\n\"\n",
    "    \"# Option c\\n\"\n",
    "    \"{option_c}\\n\\n\"\n",
    "    \"# Option d\\n\"\n",
    "    \"{option_d}\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# BACKGROUND INFORMATION OF YOUR ROLE:\\nYou are an expert in machine learning.\\nYou are verifying if a question from a bank of question is ok to be used in a job interview.\\nyour task is to analyse the provided question to verify if there is some problem.\\nYou shoud verify if there is just one option correct or if there is more than one.\\nThe question could be perfect fine, so you could say it too.\\nAlso, you should give a detailed explanation for your answer.\\n# QUESTION\\n\\n0    Which term represents correctly classified pos...\\nName: question, dtype: object\\n\\n# Option a0    True Positives\\nName: answer_a, dtype: object\\n\\n# Option b0    False Positives\\nName: answer_b, dtype: object\\n\\n# Option c0    False Negatives\\nName: answer_c, dtype: object\\n\\n# Option d0    True Negatives\\nName: answer_d, dtype: object\\n\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_revision_fromatted = prompt_revision.format(\n",
    "        question=question[\"question\"],\n",
    "        option_a=question[\"answer_a\"],\n",
    "        option_b=question[\"answer_b\"],\n",
    "        option_c=question[\"answer_c\"],\n",
    "        option_d=question[\"answer_d\"],        \n",
    "    )\n",
    "\n",
    "prompt_revision_fromatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"{\\\"correct_answer\\\": \\\"a\\\", \\\"just_one_option_correct\\\": true, \\\"option_a_explanation\\\": \\\"True Positives (TP) represent correctly classified positive instances. In the context of machine learning classification, a positive instance is a data point that belongs to the target class.  When the model correctly identifies a positive instance, it's considered a true positive.  This is an important metric used in evaluating the performance of classification models.\\\", \\\"option_b_explanation\\\": \\\"False Positives (FP), also known as Type I errors, occur when the model incorrectly classifies a negative instance as positive.  For example, if a spam filter classifies a legitimate email as spam, that would be a false positive. False Positives are also a key metric for evaluating model performance.\\\", \\\"option_c_explanation\\\": \\\"False Negatives (FN), also known as Type II errors, occur when the model incorrectly classifies a positive instance as negative. For example, if a cancer screening test fails to detect cancer in a patient who actually has it, that would be a false negative. False Negatives can be particularly problematic in situations where incorrect predictions can have serious consequences.\\\", \\\"option_d_explanation\\\": \\\"True Negatives (TN) represent correctly classified negative instances. In the context of machine learning classification, a negative instance is a data point that does not belong to the target class. When the model correctly identifies a negative instance, it's considered a true negative. This is another important metric for evaluating model performance, particularly in situations where it's crucial to correctly identify negative instances.\\\"} \"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 194,\n",
       "        \"candidates_token_count\": 328,\n",
       "        \"total_token_count\": 522\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm_gemini.generate_content(prompt_revision_fromatted)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correct_answer': 'a',\n",
       " 'just_one_option_correct': True,\n",
       " 'option_a_explanation': \"True Positives (TP) represent correctly classified positive instances. In the context of machine learning classification, a positive instance is a data point that belongs to the target class.  When the model correctly identifies a positive instance, it's considered a true positive.  This is an important metric used in evaluating the performance of classification models.\",\n",
       " 'option_b_explanation': 'False Positives (FP), also known as Type I errors, occur when the model incorrectly classifies a negative instance as positive.  For example, if a spam filter classifies a legitimate email as spam, that would be a false positive. False Positives are also a key metric for evaluating model performance.',\n",
       " 'option_c_explanation': 'False Negatives (FN), also known as Type II errors, occur when the model incorrectly classifies a positive instance as negative. For example, if a cancer screening test fails to detect cancer in a patient who actually has it, that would be a false negative. False Negatives can be particularly problematic in situations where incorrect predictions can have serious consequences.',\n",
       " 'option_d_explanation': \"True Negatives (TN) represent correctly classified negative instances. In the context of machine learning classification, a negative instance is a data point that does not belong to the target class. When the model correctly identifies a negative instance, it's considered a true negative. This is another important metric for evaluating model performance, particularly in situations where it's crucial to correctly identify negative instances.\"}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemeni_analysis = json_repair.loads(response.text)\n",
    "gemeni_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplanationSchema(BaseModel):\n",
    "    option_a_explanation: str = Field(..., description=\"Detailed explanation of why the option 'a' is correct or not.\")\n",
    "    option_b_explanation: str = Field(..., description=\"Detailed explanation of why the option 'b' is correct or not.\")\n",
    "    option_c_explanation: str = Field(..., description=\"Detailed explanation of why the option 'c' is correct or not.\")\n",
    "    option_d_explanation: str = Field(..., description=\"Detailed explanation of why the option 'd' is correct or not.\")\n",
    "    just_one_option_correct: bool = Field(..., description=\"If there is just one option correct.\")\n",
    "    correct_answer: Literal['a', 'b', 'c', 'd'] = Field(..., description=\"The correct option. It could be 'a', 'b', 'c', or 'd'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "key_groq = os.environ[\"GROQ_API_KEY\"]\n",
    "client = Groq(api_key=key_groq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# BACKGROUND INFORMATION OF YOUR ROLE:\\nYou are an expert in machine learning.\\nYou are verifying if a question from a bank of question is ok to be used in a job interview.\\nyour task is to analyse the provided question to verify if there is some problem.\\nYou shoud verify if there is just one option correct or if there is more than one.\\nThe question could be perfect fine, so you could say it too.\\nAlso, you should give a detailed explanation for your answer.\\n# QUESTION\\n\\n{question}\\n\\n# Option a\\n{option_a}\\n\\n# Option b\\n{option_b}\\n\\n# Option c\\n{option_c}\\n\\n# Option d\\n{option_d}\\n\\n# OUTPUT FORMAT: json\\n\\n{output_format}'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_revision_with_json = prompt_revision + (\n",
    "    \"# OUTPUT FORMAT: json\\n\\n\"\n",
    "    \"{output_format}\"\n",
    ")\n",
    "\n",
    "prompt_revision_with_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# BACKGROUND INFORMATION OF YOUR ROLE:\n",
      "You are an expert in machine learning.\n",
      "You are verifying if a question from a bank of question is ok to be used in a job interview.\n",
      "your task is to analyse the provided question to verify if there is some problem.\n",
      "You shoud verify if there is just one option correct or if there is more than one.\n",
      "The question could be perfect fine, so you could say it too.\n",
      "Also, you should give a detailed explanation for your answer.\n",
      "# QUESTION\n",
      "\n",
      "Which term represents correctly classified positive cases?\n",
      "\n",
      "# Option a\n",
      "True Positives\n",
      "\n",
      "# Option b\n",
      "False Positives\n",
      "\n",
      "# Option c\n",
      "False Negatives\n",
      "\n",
      "# Option d\n",
      "True Negatives\n",
      "\n",
      "# OUTPUT FORMAT: json\n",
      "\n",
      "{\n",
      "  \"properties\": {\n",
      "    \"option_a_explanation\": {\n",
      "      \"description\": \"Detailed explanation of why the option 'a' is correct or not.\",\n",
      "      \"title\": \"Option A Explanation\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"option_b_explanation\": {\n",
      "      \"description\": \"Detailed explanation of why the option 'b' is correct or not.\",\n",
      "      \"title\": \"Option B Explanation\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"option_c_explanation\": {\n",
      "      \"description\": \"Detailed explanation of why the option 'c' is correct or not.\",\n",
      "      \"title\": \"Option C Explanation\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"option_d_explanation\": {\n",
      "      \"description\": \"Detailed explanation of why the option 'd' is correct or not.\",\n",
      "      \"title\": \"Option D Explanation\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"just_one_option_correct\": {\n",
      "      \"description\": \"If there is just one option correct.\",\n",
      "      \"title\": \"Just One Option Correct\",\n",
      "      \"type\": \"boolean\"\n",
      "    },\n",
      "    \"correct_answer\": {\n",
      "      \"description\": \"The correct option. It could be 'a', 'b', 'c', or 'd'.\",\n",
      "      \"enum\": [\n",
      "        \"a\",\n",
      "        \"b\",\n",
      "        \"c\",\n",
      "        \"d\"\n",
      "      ],\n",
      "      \"title\": \"Correct Answer\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"option_a_explanation\",\n",
      "    \"option_b_explanation\",\n",
      "    \"option_c_explanation\",\n",
      "    \"option_d_explanation\",\n",
      "    \"just_one_option_correct\",\n",
      "    \"correct_answer\"\n",
      "  ],\n",
      "  \"title\": \"ExplanationSchema\",\n",
      "  \"type\": \"object\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt_revision_fromatted = prompt_revision_with_json.format(\n",
    "        question=question[\"question\"],\n",
    "        option_a=question[\"answer_a\"],\n",
    "        option_b=question[\"answer_b\"],\n",
    "        option_c=question[\"answer_c\"],\n",
    "        option_d=question[\"answer_d\"],\n",
    "        output_format=json.dumps(ExplanationSchema.model_json_schema(), indent=2)\n",
    "    )\n",
    "\n",
    "print( prompt_revision_fromatted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    # model=\"llama3-8b-8192\",\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt_revision_fromatted\n",
    "    }],\n",
    "    temperature=0.5,\n",
    "    # max_tokens=2420,\n",
    "    top_p=0.95,\n",
    "    stream=False,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "completion = response.choices[0].message\n",
    "question = json_repair.loads(completion.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'option_a_explanation': \"Option 'a' is correct because True Positives represent the number of positive cases that were correctly classified as positive.\",\n",
       " 'option_b_explanation': \"Option 'b' is incorrect because False Positives represent the number of negative cases that were incorrectly classified as positive.\",\n",
       " 'option_c_explanation': \"Option 'c' is incorrect because False Negatives represent the number of positive cases that were incorrectly classified as negative.\",\n",
       " 'option_d_explanation': \"Option 'd' is incorrect because True Negatives represent the number of negative cases that were correctly classified as negative.\",\n",
       " 'just_one_option_correct': True,\n",
       " 'correct_answer': 'a'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
