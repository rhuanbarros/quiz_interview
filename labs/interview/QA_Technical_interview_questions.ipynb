{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/quiz_interview/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from google.ai.generativelanguage_v1beta.types import content\n",
    "\n",
    "key_gemini = os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "genai.configure(api_key=key_gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "generation_config = {\n",
    "    \"temperature\": 0.75,\n",
    "    \"top_p\": 0.95,\n",
    "    # \"top_k\": 64,\n",
    "    # \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "    # \"response_schema\": schema,\n",
    "    # \"response_mime_type\": \"application/json\",\n",
    "}\n",
    "    \n",
    "\n",
    "llm_gemini = genai.GenerativeModel(\n",
    "    # model_name=\"gemini-1.5-flash\",\n",
    "    # model_name=\"gemini-1.5-flash-exp-0827\",\n",
    "    # model_name=\"gemini-1.5-pro-002\",\n",
    "    model_name=\"gemini-2.0-flash-exp\",\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"KNOLEDGE LIST:\\n\\n\"\n",
    "    \"{KNOLEDGE}\\n\\n\"\n",
    "    \"I'm preparing for a MACHINE LEARNING job interview. I need you to help me to create some questions to practice\\n\"\n",
    "    \"given the KNOLEDGE LIST above, create 10 questions and answers.\\n\"\n",
    "    \"Your outuput shoud be strucutured in this way: QUESTION, ANSWER_NARRATIVE, ANSWER_BULLET_LIST\\n\"\n",
    "    \"ANSWER_NARRATIVE should be a complete answer in 1 or 2 paragraphs in the best way a condidate should answer it in a interview.\\n\"\n",
    "    \"ANSWER_BULLET_LIST should be a bullet list with 5 itens, each item could have around 15 words.\\n\"\n",
    "    # \"each item should be create in a way to maximize the memorization condering the ANSWER_NARRATIVE.\\n\"\n",
    "    \"write everything in english\\n\"\n",
    "    \"output it in a way be formated to use with google docs to copy it to paste in the app.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOLEDGE = \"\"\"\n",
    "Training Techniques\n",
    "\t1. Pre-training Basics\n",
    "\t2. Self-Supervised Learning\n",
    "\t3. Optimization Algorithms\n",
    "\t\t○ Common optimizers (Adam, AdamW, RMSprop).\n",
    "\t\t○ Importance of learning rate scheduling.\n",
    "\t\t○ Gradient clipping and handling exploding/vanishing gradients.\n",
    "\t4. Distributed Training\n",
    "\t\t○ Techniques like data parallelism, model parallelism, and pipeline parallelism.\n",
    "\t\t○ Challenges in distributed training (e.g., synchronization, communication overhead).\n",
    "\t5. Scaling Laws\n",
    "\t\t○ Relationship between model size, training data, and performance.\n",
    "Insights from studies on scaling LLMs (e.g., OpenAI, DeepMind).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNOLEDGE LIST:\n",
      "\n",
      "\n",
      "Training Techniques\n",
      "\t1. Pre-training Basics\n",
      "\t2. Self-Supervised Learning\n",
      "\t3. Optimization Algorithms\n",
      "\t\t○ Common optimizers (Adam, AdamW, RMSprop).\n",
      "\t\t○ Importance of learning rate scheduling.\n",
      "\t\t○ Gradient clipping and handling exploding/vanishing gradients.\n",
      "\t4. Distributed Training\n",
      "\t\t○ Techniques like data parallelism, model parallelism, and pipeline parallelism.\n",
      "\t\t○ Challenges in distributed training (e.g., synchronization, communication overhead).\n",
      "\t5. Scaling Laws\n",
      "\t\t○ Relationship between model size, training data, and performance.\n",
      "Insights from studies on scaling LLMs (e.g., OpenAI, DeepMind).\n",
      "\n",
      "\n",
      "\n",
      "I'm preparing for a MACHINE LEARNING job interview. I need you to help me to create some questions to practice\n",
      "given the KNOLEDGE LIST above, create 10 questions and answers.\n",
      "Your outuput shoud be strucutured in this way: QUESTION, ANSWER_NARRATIVE, ANSWER_BULLET_LIST\n",
      "ANSWER_NARRATIVE should be a complete answer in 1 or 2 paragraphs in the best way a condidate should answer it in a interview.\n",
      "ANSWER_BULLET_LIST should be a bullet list with 5 itens, each item could have around 15 words.\n",
      "write everything in english\n",
      "output it in a way be formated to use with google docs to copy it to paste in the app.\n"
     ]
    }
   ],
   "source": [
    "prompt_formatted = prompt.format(\n",
    "    KNOLEDGE=KNOLEDGE\n",
    "    )\n",
    "\n",
    "print( prompt_formatted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = llm_gemini.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here are 10 interview practice questions and answers based on your provided knowledge list, formatted for easy copying into Google Docs:\n",
      "\n",
      "**Question 1:**\n",
      "\n",
      "What are some key considerations when preparing data for pre-training a large language model?\n",
      "\n",
      "**Answer_Narrative:**\n",
      "\n",
      "Preparing data for pre-training a large language model is crucial for the model's success. It involves several key steps, starting with collecting a massive, diverse dataset that accurately reflects the real-world text distributions the model will encounter. This data should be cleaned and preprocessed to remove noise and inconsistencies. Attention must be paid to data quality, handling biases, and ensuring that the data covers a wide range of topics and linguistic styles. Furthermore, tokenization, which involves converting text into numerical inputs, is a fundamental step that affects the model’s efficiency and performance.\n",
      "\n",
      "**Answer_Bullet_List:**\n",
      "\n",
      "*   Data collection should prioritize diversity and represent real-world text distributions.\n",
      "*   Thorough cleaning and preprocessing steps are required to remove noise and inconsistencies in the data.\n",
      "*   Bias identification and mitigation are essential to ensure fair and reliable model performance.\n",
      "*   Tokenization is a critical step for transforming text into numerical representations.\n",
      "*   Data quality has a direct effect on the model's learning capabilities and final performance.\n",
      "\n",
      "**Question 2:**\n",
      "\n",
      "Explain the concept of self-supervised learning and why it's important for pre-training large language models.\n",
      "\n",
      "**Answer_Narrative:**\n",
      "\n",
      "Self-supervised learning is a training paradigm where a model learns from unlabeled data by creating its own labels. This is vital for pre-training large language models because it allows these models to learn rich representations of language from vast amounts of text without the need for extensive human annotation. Techniques like masked language modeling (BERT) and next-sentence prediction are common examples. By learning to predict masked words or relationships between sentences, the model develops a deep understanding of syntax, semantics, and contextual information.\n",
      "\n",
      "**Answer_Bullet_List:**\n",
      "\n",
      "*   Self-supervised learning enables models to learn from unlabeled data by creating their own labels.\n",
      "*   This approach is crucial for pre-training, especially with the large datasets used for LLMs.\n",
      "*   It allows models to learn rich language representations without expensive human annotations.\n",
      "*   Masked language modeling and next sentence prediction are common techniques.\n",
      "*   The model learns syntax, semantics, and contextual information through these self-generated tasks.\n",
      "\n",
      "**Question 3:**\n",
      "\n",
      "Compare and contrast the Adam and AdamW optimizers. What are their key differences and when might you choose one over the other?\n",
      "\n",
      "**Answer_Narrative:**\n",
      "\n",
      "Both Adam and AdamW are adaptive learning rate optimization algorithms, but they differ in how they handle weight decay. Adam applies weight decay directly to the gradients, which can lead to suboptimal results. AdamW, on the other hand, decouples weight decay from the gradient update, applying it directly to the weights instead. This decoupling often leads to better generalization and improved model performance, especially for large models. Therefore, AdamW is often preferred over Adam, particularly when training large neural networks where weight decay is crucial for regularization.\n",
      "\n",
      "**Answer_Bullet_List:**\n",
      "\n",
      "*   Adam and AdamW are both adaptive learning rate optimizers that are widely used.\n",
      "*   The key difference lies in how they handle weight decay during optimization.\n",
      "*   Adam applies weight decay directly to the gradients, which can be suboptimal.\n",
      "*   AdamW decouples weight decay and applies it to the weights, improving generalization.\n",
      "*   AdamW is usually the preferred choice when training large neural networks.\n",
      "\n",
      "**Question 4:**\n",
      "\n",
      "Why is learning rate scheduling important when training neural networks, and what are some common scheduling techniques?\n",
      "\n",
      "**Answer_Narrative:**\n",
      "\n",
      "Learning rate scheduling is vital because a fixed learning rate can lead to unstable training, slow convergence, or getting stuck in suboptimal local minima. By dynamically adjusting the learning rate during training, we can fine-tune the optimization process. Common techniques include step decay, where the learning rate is reduced by a factor at specific intervals, and cosine annealing, which smoothly decreases the learning rate following a cosine curve. These schedules help the model converge more efficiently and improve final performance by allowing for larger initial steps and finer adjustments towards the end of training.\n",
      "\n",
      "**Answer_Bullet_List:**\n",
      "\n",
      "*   Fixed learning rates can cause unstable training, slow convergence, or suboptimal results.\n",
      "*   Learning rate scheduling dynamically adjusts the rate during training for better performance.\n",
      "*   Step decay reduces the rate by a factor at specific intervals during the training process.\n",
      "*   Cosine annealing smoothly decreases the learning rate following a cosine curve.\n",
      "*   These schedules allow for larger initial steps and finer tuning towards the end of training.\n",
      "\n",
      "**Question 5:**\n",
      "\n",
      "What is gradient clipping and why is it used in the training of deep neural networks?\n",
      "\n",
      "**Answer_Narrative:**\n",
      "\n",
      "Gradient clipping is a technique used to prevent exploding gradients during the training of deep neural networks. Exploding gradients occur when the gradients become excessively large, leading to unstable training, divergent updates, and poor model convergence. Gradient clipping works by setting a maximum threshold for the gradient's magnitude, and any gradient value above that threshold is scaled down to the defined limit. This ensures that the weights are not updated too drastically, and the model can continue to learn effectively.\n",
      "\n",
      "**Answer_Bullet_List:**\n",
      "\n",
      "*   Gradient clipping is used to prevent exploding gradients during neural network training.\n",
      "*   Exploding gradients can cause unstable training and poor model convergence.\n",
      "*   The technique works by setting a maximum threshold for the gradient's magnitude.\n",
      "*   Gradients exceeding the threshold are scaled down to the defined limit.\n",
      "*   This prevents drastic updates and ensures stable and effective model training.\n",
      "\n",
      "**Question 6:**\n",
      "\n",
      "Explain the concept of data parallelism in distributed training and its advantages and challenges.\n",
      "\n",
      "**Answer_Narrative:**\n",
      "\n",
      "Data parallelism involves dividing the training data into subsets and distributing these subsets across multiple devices, such as GPUs or TPUs. Each device trains a copy of the same model on its subset, and the gradients are synchronized and aggregated to update the model's parameters. The advantage is that it allows for faster training by utilizing multiple computational resources simultaneously. However, it poses challenges such as the need for efficient synchronization of gradients, which can introduce communication overhead and impact scaling, especially when using many devices.\n",
      "\n",
      "**Answer_Bullet_List:**\n",
      "\n",
      "*   Data parallelism involves distributing training data across multiple devices.\n",
      "*   Each device trains a copy of the same model on its assigned data subset.\n",
      "*   Gradients are synchronized and aggregated to update the model parameters.\n",
      "*   This allows for faster training by utilizing multiple computational resources.\n",
      "*   Challenges include synchronization overhead and communication bottlenecks.\n",
      "\n",
      "**Question 7:**\n",
      "\n",
      "What is model parallelism, and in what scenarios is it typically used?\n",
      "\n",
      "**Answer_Narrative:**\n",
      "\n",
      "Model parallelism is a distributed training technique where different parts of a neural network are assigned to different devices. This is commonly used when the model is too large to fit in the memory of a single device, as is often the case with very large language models. Model parallelism can be implemented in different ways, such as layer-wise parallelism or tensor parallelism. By splitting the model across devices, it's possible to train models that would otherwise be infeasible to train on a single machine. However, it can introduce additional complexities in terms of communication and synchronization between devices.\n",
      "\n",
      "**Answer_Bullet_List:**\n",
      "\n",
      "*   Model parallelism involves assigning different parts of a neural network to different devices.\n",
      "*   It is typically used when the model is too large to fit in a single device's memory.\n",
      "*   Techniques include layer-wise parallelism and tensor parallelism to split the model.\n",
      "*   This approach enables training of large models that are otherwise infeasible to train.\n",
      "*   However, additional communication and synchronization complexities are introduced.\n",
      "\n",
      "**Question 8:**\n",
      "\n",
      "Describe the concept of pipeline parallelism and how it differs from model parallelism.\n",
      "\n",
      "**Answer_Narrative:**\n",
      "\n",
      "Pipeline parallelism is a technique where the processing of data through a model is divided into stages, with each stage assigned to a different device. It is similar to an assembly line, where one device processes the first layers, another processes the middle layers, and so on. Unlike model parallelism, which divides layers across devices, pipeline parallelism divides the data processing flow across devices. This allows for a more efficient use of resources by keeping multiple devices active, and it's particularly useful for large models with multiple sequential stages. However, it introduces complexities in managing the flow of data and ensuring efficient resource utilization.\n",
      "\n",
      "**Answer_Bullet_List:**\n",
      "\n",
      "*   Pipeline parallelism divides the processing of data through the model into stages.\n",
      "*   Each stage of the pipeline is assigned to a different device, similar to an assembly line.\n",
      "*   It differs from model parallelism, which divides the model layers across devices.\n",
      "*   Pipeline parallelism improves resource utilization by keeping multiple devices active.\n",
      "*   It introduces complexities in managing data flow and resource efficiency.\n",
      "\n",
      "**Question 9:**\n",
      "\n",
      "What are scaling laws in the context of large language models, and why are they important?\n",
      "\n",
      "**Answer_Narrative:**\n",
      "\n",
      "Scaling laws describe the empirical relationships between model size, training data, and model performance. They suggest that as you increase the size of a model and the amount of training data, the model's performance improves predictably. Understanding scaling laws is important for planning and optimizing the training of large language models. They can guide decisions about how much data and compute power are needed to achieve a certain performance level, and they help to understand the trade-offs between model size, training cost, and model accuracy.\n",
      "\n",
      "**Answer_Bullet_List:**\n",
      "\n",
      "*   Scaling laws describe the relationship between model size, data, and performance.\n",
      "*   They suggest that increasing size and data leads to predictable performance improvements.\n",
      "*   Understanding these laws is crucial for planning and optimizing LLM training.\n",
      "*   They guide decisions about data and compute resources needed for desired performance.\n",
      "*   They help understand the trade-offs between model size, cost, and accuracy.\n",
      "\n",
      "**Question 10:**\n",
      "\n",
      "Based on insights from studies on scaling LLMs (e.g., OpenAI, DeepMind), what are some key findings that have shaped the current understanding of LLM training?\n",
      "\n",
      "**Answer_Narrative:**\n",
      "\n",
      "Studies from organizations like OpenAI and DeepMind have provided significant insights into the training of large language models. A key finding is that scaling up both model size and training data leads to substantial improvements in performance, often following power-law relationships. These studies have demonstrated that larger models can exhibit emergent abilities, such as in-context learning, that are not apparent in smaller models. They also showed that increasing compute power to train these models is vital, as well as carefully curating high-quality, diverse training data. These findings have shaped the current understanding that large, well-trained models with enough resources are necessary for achieving state-of-the-art performance in many natural language tasks.\n",
      "\n",
      "**Answer_Bullet_List:**\n",
      "\n",
      "*   Scaling model size and training data leads to substantial performance improvements.\n",
      "*   Larger models can exhibit emergent abilities such as in-context learning.\n",
      "*   Increased compute power is vital for training these large language models.\n",
      "*   Careful curation of high-quality, diverse training data is also necessary.\n",
      "*   These findings emphasize the importance of large, well-trained models with enough resources.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(prompt_formatted)\n",
    "print( response.text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
