{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question revision chain\n",
    "\n",
    "- get a question\n",
    "- LLM A and B verify if the question is OK\n",
    "    - there is more than one option correct\n",
    "    - sugest a detailed explanation to each option\n",
    "- verify if correct_answer is equal for both\n",
    "    - if YES: \n",
    "        - verify if there is just one option true:\n",
    "            - if YES: \n",
    "                - just take the explanations from gemini and set the correct answer\n",
    "            - if NO: \n",
    "                - take a vote from openai\n",
    "                - go to the part of save the question because it should have a majority vote for one option\n",
    "    - if NO: \n",
    "        - if it is not passing this part for the second time\n",
    "            - if YES:\n",
    "                - fix the options\n",
    "                - go to the start of flow\n",
    "            - if NO: \n",
    "                - discart this question because it already tried to fix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json_repair\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from typing_extensions import TypedDict\n",
    "# from typing import Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 953 entries, 0 to 952\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   id                 953 non-null    int64 \n",
      " 1   created_at         953 non-null    object\n",
      " 2   subject_matter     953 non-null    object\n",
      " 3   topic_description  953 non-null    object\n",
      " 4   level              953 non-null    object\n",
      " 5   question           953 non-null    object\n",
      " 6   type               953 non-null    object\n",
      " 7   answer_correct     953 non-null    object\n",
      " 8   explanation        953 non-null    object\n",
      " 9   answer_a           953 non-null    object\n",
      " 10  answer_b           953 non-null    object\n",
      " 11  answer_c           953 non-null    object\n",
      " 12  answer_d           953 non-null    object\n",
      "dtypes: int64(1), object(12)\n",
      "memory usage: 96.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"questions.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                  11\n",
       "created_at                               2024-09-22 12:31:43.546285+00\n",
       "subject_matter                                             Probability\n",
       "topic_description                       Understanding Confusion Matrix\n",
       "level                                                     1 - Remember\n",
       "question             Which term represents correctly classified pos...\n",
       "type                                                  multiple_options\n",
       "answer_correct                                                       a\n",
       "explanation          True Positives (TP) represent the number of ca...\n",
       "answer_a                                                True Positives\n",
       "answer_b                                               False Positives\n",
       "answer_c                                               False Negatives\n",
       "answer_d                                                True Negatives\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = df.iloc[0]\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which term represents correctly classified positive cases?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question[\"question\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/quiz_interview/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from google.ai.generativelanguage_v1beta.types import content\n",
    "\n",
    "key_gemini = os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "genai.configure(api_key=key_gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = content.Schema(\n",
    "    type=content.Type.OBJECT,\n",
    "    properties={\n",
    "        \"option_a_explanation\": content.Schema(\n",
    "            type=content.Type.STRING,\n",
    "            description=\"Detailed explanation of why the option 'a' is correct or not.\",\n",
    "        ),\n",
    "        \"option_b_explanation\": content.Schema(\n",
    "            type=content.Type.STRING,\n",
    "            description=\"Detailed explanation of why the option 'b' is correct or not.\",\n",
    "        ),\n",
    "        \"option_c_explanation\": content.Schema(\n",
    "            type=content.Type.STRING,\n",
    "            description=\"Detailed explanation of why the option 'c' is correct or not.\",\n",
    "        ),\n",
    "        \"option_d_explanation\": content.Schema(\n",
    "            type=content.Type.STRING,\n",
    "            description=\"Detailed explanation of why the option 'd' is correct or not.\",\n",
    "        ),\n",
    "        \"just_one_option_correct\": content.Schema(\n",
    "            type=content.Type.BOOLEAN,\n",
    "            description=\"If there is just one option correct.\",\n",
    "        ),\n",
    "        \"correct_answer\": content.Schema(\n",
    "            type=content.Type.STRING,\n",
    "            description=\"The correct option. It could be 'a', 'b', 'c', 'd'.\",\n",
    "        ),\n",
    "    },\n",
    "    required=[\n",
    "        \"option_a_explanation\",\n",
    "        \"option_b_explanation\",\n",
    "        \"option_c_explanation\",\n",
    "        \"option_d_explanation\",\n",
    "        \"just_one_option_correct\",\n",
    "        \"correct_answer\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "generation_config = {\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.95,\n",
    "    # \"top_k\": 64,\n",
    "    # \"max_output_tokens\": 8192,\n",
    "    \"response_schema\": schema,\n",
    "    \"response_mime_type\": \"application/json\",\n",
    "}\n",
    "    \n",
    "\n",
    "llm_gemini = genai.GenerativeModel(\n",
    "    # model_name=\"gemini-1.5-flash\",\n",
    "    model_name=\"gemini-1.5-flash-exp-0827\",\n",
    "    # model_name=\"gemini-1.5-pro-exp-0827\",\n",
    "    generation_config=generation_config,\n",
    "    # safety_settings = Adjust safety settings\n",
    "    # See https://ai.google.dev/gemini-api/docs/safety-settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_revision = (\n",
    "    \"# BACKGROUND INFORMATION OF YOUR ROLE:\\n\"\n",
    "    \"You are an expert in machine learning.\\n\"\n",
    "    \"You are verifying if a question from a bank of question is ok to be used in a job interview.\\n\"\n",
    "    \"your task is to analyse the provided question to verify if there is some problem.\\n\"\n",
    "    \"You shoud verify if there is just one option correct or if there is more than one.\\n\"\n",
    "    \"The question could be perfect fine, so you could say it too.\\n\"\n",
    "    \"Also, you should give a detailed explanation for your answer.\\n\"\n",
    "    \"# QUESTION\\n\\n\"\n",
    "    \"{question}\\n\\n\"\n",
    "    \"# Option a\\n\"\n",
    "    \"{option_a}\\n\\n\"\n",
    "    \"# Option b\\n\"\n",
    "    \"{option_b}\\n\\n\"\n",
    "    \"# Option c\\n\"\n",
    "    \"{option_c}\\n\\n\"\n",
    "    \"# Option d\\n\"\n",
    "    \"{option_d}\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# BACKGROUND INFORMATION OF YOUR ROLE:\n",
      "You are an expert in machine learning.\n",
      "You are verifying if a question from a bank of question is ok to be used in a job interview.\n",
      "your task is to analyse the provided question to verify if there is some problem.\n",
      "You shoud verify if there is just one option correct or if there is more than one.\n",
      "The question could be perfect fine, so you could say it too.\n",
      "Also, you should give a detailed explanation for your answer.\n",
      "# QUESTION\n",
      "\n",
      "Which term represents correctly classified positive cases?\n",
      "\n",
      "# Option a\n",
      "True Positives\n",
      "\n",
      "# Option b\n",
      "False Positives\n",
      "\n",
      "# Option c\n",
      "False Negatives\n",
      "\n",
      "# Option d\n",
      "True Negatives\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_revision_formatted = prompt_revision.format(\n",
    "        question=question[\"question\"],\n",
    "        option_a=question[\"answer_a\"],\n",
    "        option_b=question[\"answer_b\"],\n",
    "        option_c=question[\"answer_c\"],\n",
    "        option_d=question[\"answer_d\"],        \n",
    "    )\n",
    "\n",
    "print( prompt_revision_formatted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"{\\\"correct_answer\\\": \\\"a\\\", \\\"just_one_option_correct\\\": true, \\\"option_a_explanation\\\": \\\"True Positives (TP) represent the cases where the model correctly predicts a positive outcome when the actual outcome is also positive. It is a measure of how well the model identifies positive cases.\\\", \\\"option_b_explanation\\\": \\\"False Positives (FP) represent the cases where the model incorrectly predicts a positive outcome when the actual outcome is negative. It is a measure of how often the model incorrectly identifies a positive case.\\\", \\\"option_c_explanation\\\": \\\"False Negatives (FN) represent the cases where the model incorrectly predicts a negative outcome when the actual outcome is positive. It is a measure of how often the model fails to identify a positive case.\\\", \\\"option_d_explanation\\\": \\\"True Negatives (TN) represent the cases where the model correctly predicts a negative outcome when the actual outcome is also negative. It is a measure of how well the model identifies negative cases.\\\"} \"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 149,\n",
       "        \"candidates_token_count\": 207,\n",
       "        \"total_token_count\": 356\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm_gemini.generate_content(prompt_revision_formatted)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correct_answer': 'a',\n",
       " 'just_one_option_correct': True,\n",
       " 'option_a_explanation': 'True Positives (TP) represent the cases where the model correctly predicts a positive outcome when the actual outcome is also positive. It is a measure of how well the model identifies positive cases.',\n",
       " 'option_b_explanation': 'False Positives (FP) represent the cases where the model incorrectly predicts a positive outcome when the actual outcome is negative. It is a measure of how often the model incorrectly identifies a positive case.',\n",
       " 'option_c_explanation': 'False Negatives (FN) represent the cases where the model incorrectly predicts a negative outcome when the actual outcome is positive. It is a measure of how often the model fails to identify a positive case.',\n",
       " 'option_d_explanation': 'True Negatives (TN) represent the cases where the model correctly predicts a negative outcome when the actual outcome is also negative. It is a measure of how well the model identifies negative cases.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_analysis = json_repair.loads(response.text)\n",
    "gemini_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplanationSchema(BaseModel):\n",
    "    option_a_explanation: str = Field(..., description=\"Detailed explanation of why the option 'a' is correct or not.\")\n",
    "    option_b_explanation: str = Field(..., description=\"Detailed explanation of why the option 'b' is correct or not.\")\n",
    "    option_c_explanation: str = Field(..., description=\"Detailed explanation of why the option 'c' is correct or not.\")\n",
    "    option_d_explanation: str = Field(..., description=\"Detailed explanation of why the option 'd' is correct or not.\")\n",
    "    just_one_option_correct: bool = Field(..., description=\"If there is just one option correct.\")\n",
    "    correct_answer: Literal['a', 'b', 'c', 'd'] = Field(..., description=\"The correct option. It could be 'a', 'b', 'c', or 'd'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "key_groq = os.environ[\"GROQ_API_KEY\"]\n",
    "client = Groq(api_key=key_groq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_revision_with_json = prompt_revision + (\n",
    "    \"# OUTPUT FORMAT: json\\n\\n\"\n",
    "    \"{output_format}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# BACKGROUND INFORMATION OF YOUR ROLE:\n",
      "You are an expert in machine learning.\n",
      "You are verifying if a question from a bank of question is ok to be used in a job interview.\n",
      "your task is to analyse the provided question to verify if there is some problem.\n",
      "You shoud verify if there is just one option correct or if there is more than one.\n",
      "The question could be perfect fine, so you could say it too.\n",
      "Also, you should give a detailed explanation for your answer.\n",
      "# QUESTION\n",
      "\n",
      "Which term represents correctly classified positive cases?\n",
      "\n",
      "# Option a\n",
      "True Positives\n",
      "\n",
      "# Option b\n",
      "False Positives\n",
      "\n",
      "# Option c\n",
      "False Negatives\n",
      "\n",
      "# Option d\n",
      "True Negatives\n",
      "\n",
      "# OUTPUT FORMAT: json\n",
      "\n",
      "{\n",
      "  \"properties\": {\n",
      "    \"option_a_explanation\": {\n",
      "      \"description\": \"Detailed explanation of why the option 'a' is correct or not.\",\n",
      "      \"title\": \"Option A Explanation\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"option_b_explanation\": {\n",
      "      \"description\": \"Detailed explanation of why the option 'b' is correct or not.\",\n",
      "      \"title\": \"Option B Explanation\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"option_c_explanation\": {\n",
      "      \"description\": \"Detailed explanation of why the option 'c' is correct or not.\",\n",
      "      \"title\": \"Option C Explanation\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"option_d_explanation\": {\n",
      "      \"description\": \"Detailed explanation of why the option 'd' is correct or not.\",\n",
      "      \"title\": \"Option D Explanation\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"just_one_option_correct\": {\n",
      "      \"description\": \"If there is just one option correct.\",\n",
      "      \"title\": \"Just One Option Correct\",\n",
      "      \"type\": \"boolean\"\n",
      "    },\n",
      "    \"correct_answer\": {\n",
      "      \"description\": \"The correct option. It could be 'a', 'b', 'c', or 'd'.\",\n",
      "      \"enum\": [\n",
      "        \"a\",\n",
      "        \"b\",\n",
      "        \"c\",\n",
      "        \"d\"\n",
      "      ],\n",
      "      \"title\": \"Correct Answer\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"option_a_explanation\",\n",
      "    \"option_b_explanation\",\n",
      "    \"option_c_explanation\",\n",
      "    \"option_d_explanation\",\n",
      "    \"just_one_option_correct\",\n",
      "    \"correct_answer\"\n",
      "  ],\n",
      "  \"title\": \"ExplanationSchema\",\n",
      "  \"type\": \"object\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt_revision_formatted = prompt_revision_with_json.format(\n",
    "        question=question[\"question\"],\n",
    "        option_a=question[\"answer_a\"],\n",
    "        option_b=question[\"answer_b\"],\n",
    "        option_c=question[\"answer_c\"],\n",
    "        option_d=question[\"answer_d\"],\n",
    "        output_format=json.dumps(ExplanationSchema.model_json_schema(), indent=2)\n",
    "    )\n",
    "\n",
    "print( prompt_revision_formatted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'option_a_explanation': 'This option is correct because True Positives represent the cases that are correctly classified as positive. In the context of machine learning, this term is used to describe the number of instances that are correctly predicted as belonging to the positive class.',\n",
       " 'option_b_explanation': 'This option is incorrect because False Positives represent the cases that are incorrectly classified as positive. In other words, these are the instances that do not belong to the positive class but are predicted as such.',\n",
       " 'option_c_explanation': 'This option is incorrect because False Negatives represent the cases that are incorrectly classified as negative. In other words, these are the instances that belong to the positive class but are predicted as not belonging to it.',\n",
       " 'option_d_explanation': 'This option is incorrect because True Negatives represent the cases that are correctly classified as negative. In other words, these are the instances that do not belong to the positive class and are correctly predicted as such.',\n",
       " 'just_one_option_correct': True,\n",
       " 'correct_answer': 'a'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    # model=\"llama3-8b-8192\",\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt_revision_formatted\n",
    "    }],\n",
    "    temperature=0.5,\n",
    "    # max_tokens=2420,\n",
    "    top_p=0.95,\n",
    "    stream=False,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "completion = response.choices[0].message\n",
    "llama_analysis = json_repair.loads(completion.content)\n",
    "llama_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_merge = (\n",
    "#     \"# BACKGROUND INFORMATION OF YOUR ROLE:\\n\"\n",
    "#     \"You are an expert in machine learning.\\n\"\n",
    "#     \"You working (with some other professionals) verifying if a question from a bank of question is ok to be used in a job interview.\\n\"\n",
    "#     \"The question has already been analized by other professionals. Their analysis is below.\\n\"\n",
    "#     \"# YOUR TASK\\n\"\n",
    "#     \"your task is to give a detailed explanation for each option taken in consideration the analysis from the experts.\\n\"\n",
    "#     \"You should also take in consideration the choosen\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliate question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# BACKGROUND INFORMATION OF YOUR ROLE:\n",
      "You are an expert in machine learning.\n",
      "You are verifying if a question from a bank of question is ok to be used in a job interview.\n",
      "your task is to analyse the provided question to verify if there is some problem.\n",
      "You shoud verify if there is just one option correct or if there is more than one.\n",
      "The question could be perfect fine, so you could say it too.\n",
      "Also, you should give a detailed explanation for your answer.\n",
      "# QUESTION\n",
      "\n",
      "Which term represents correctly classified positive cases?\n",
      "\n",
      "# Option a\n",
      "True Positives\n",
      "\n",
      "# Option b\n",
      "False Positives\n",
      "\n",
      "# Option c\n",
      "False Negatives\n",
      "\n",
      "# Option d\n",
      "True Negatives\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_revision_formatted = prompt_revision.format(\n",
    "        question=question[\"question\"],\n",
    "        option_a=question[\"answer_a\"],\n",
    "        option_b=question[\"answer_b\"],\n",
    "        option_c=question[\"answer_c\"],\n",
    "        option_d=question[\"answer_d\"],\n",
    "        # output_format=json.dumps(ExplanationSchema.model_json_schema(), indent=2)\n",
    "    )\n",
    "\n",
    "print( prompt_revision_formatted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_revision_formatted},\n",
    "        # {\"role\": \"user\", \"content\": \"hello\"}\n",
    "    ],\n",
    "    response_format=ExplanationSchema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExplanationSchema(option_a_explanation='True Positives (TP) are the results that indicate the presence of a condition (positive cases) correctly identified by a classification model. In binary classification, when we say a case is a true positive, it means that the model has correctly predicted the positive class, thus, this is the correct definition for correctly classified positive cases.', option_b_explanation='False Positives (FP) refer to the instances where the model predicts the positive class incorrectly; that is, the model predicted a positive when the actual case was negative. This does not represent correctly classified positive cases, hence this option is incorrect.', option_c_explanation='False Negatives (FN) are instances where the model fails to identify a positive case and predicts it as negative. This results in missing a positive case, and thus, is the opposite of correctly classified positive cases. Therefore, this option is also incorrect.', option_d_explanation='True Negatives (TN) represent instances where the model correctly predicts the negative class. While this is a correct classification, it does not pertain to positive cases—hence, this option does not represent correctly classified positive cases either.', just_one_option_correct=True, correct_answer='a')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.ExplanationSchema"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(completion.choices[0].message.parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True Positives (TP) are the results that indicate the presence of a condition (positive cases) correctly identified by a classification model. In binary classification, when we say a case is a true positive, it means that the model has correctly predicted the positive class, thus, this is the correct definition for correctly classified positive cases.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].message.parsed.option_a_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'option_a_explanation': 'True Positives (TP) are the results that indicate the presence of a condition (positive cases) correctly identified by a classification model. In binary classification, when we say a case is a true positive, it means that the model has correctly predicted the positive class, thus, this is the correct definition for correctly classified positive cases.',\n",
       " 'option_b_explanation': 'False Positives (FP) refer to the instances where the model predicts the positive class incorrectly; that is, the model predicted a positive when the actual case was negative. This does not represent correctly classified positive cases, hence this option is incorrect.',\n",
       " 'option_c_explanation': 'False Negatives (FN) are instances where the model fails to identify a positive case and predicts it as negative. This results in missing a positive case, and thus, is the opposite of correctly classified positive cases. Therefore, this option is also incorrect.',\n",
       " 'option_d_explanation': 'True Negatives (TN) represent instances where the model correctly predicts the negative class. While this is a correct classification, it does not pertain to positive cases—hence, this option does not represent correctly classified positive cases either.',\n",
       " 'just_one_option_correct': True,\n",
       " 'correct_answer': 'a'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_openai = json_repair.loads( completion.choices[0].message.parsed.model_dump_json() )\n",
    "analysis_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'option_a_explanation': 'True Positives (TP) are the results that indicate the presence of a condition (positive cases) correctly identified by a classification model. In binary classification, when we say a case is a true positive, it means that the model has correctly predicted the positive class, thus, this is the correct definition for correctly classified positive cases.',\n",
       " 'option_b_explanation': 'False Positives (FP) refer to the instances where the model predicts the positive class incorrectly; that is, the model predicted a positive when the actual case was negative. This does not represent correctly classified positive cases, hence this option is incorrect.',\n",
       " 'option_c_explanation': 'False Negatives (FN) are instances where the model fails to identify a positive case and predicts it as negative. This results in missing a positive case, and thus, is the opposite of correctly classified positive cases. Therefore, this option is also incorrect.',\n",
       " 'option_d_explanation': 'True Negatives (TN) represent instances where the model correctly predicts the negative class. While this is a correct classification, it does not pertain to positive cases—hence, this option does not represent correctly classified positive cases either.',\n",
       " 'just_one_option_correct': True,\n",
       " 'correct_answer': 'a'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to fix question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_fix_question = (\n",
    "    \"# BACKGROUND INFORMATION OF YOUR ROLE:\\n\"\n",
    "    \"You are an expert in machine learning.\\n\"\n",
    "    \"You working (with some other professionals) verifying if a question from a bank of question is ok to be used in a job interview.\\n\"\n",
    "    \"The question has already been analized by other professionals. Their analysis is below.\\n\"\n",
    "    \"It was already pointed that this question could have more than one option as correct, but it shouldn't. It should have just one correct option.\\n\"\n",
    "    \"# YOUR TASK\\n\"\n",
    "    \"your task is to fix the questions and/or the options in a way that it mantain the overall ideia for this question, but it should have after your correction, just one option as correct.\\n\"\n",
    "    \"Also, you should give a detailed explanation for your answer.\\n\\n\"\n",
    "    \"# ORIGINAL QUESTION\\n\\n\"\n",
    "    \"{question}\\n\\n\"\n",
    "    \"# Option a\\n\"\n",
    "    \"{option_a}\\n\\n\"\n",
    "    \"# Option b\\n\"\n",
    "    \"{option_b}\\n\\n\"\n",
    "    \"# Option c\\n\"\n",
    "    \"{option_c}\\n\\n\"\n",
    "    \"# Option d\\n\"\n",
    "    \"{option_d}\\n\\n\"\n",
    "\n",
    "    \"# PROFESSIONAL ONE ANALYSIS\\n\"\n",
    "    \"option_a_explanation\\n\"\n",
    "    \"{option_a_explanation}\\n\"\n",
    "    \"option_b_explanation\\n\"\n",
    "    \"{option_b_explanation}\\n\"\n",
    "    \"option_c_explanation\\n\"\n",
    "    \"{option_c_explanation}\\n\"\n",
    "    \"option_d_explanation\\n\"\n",
    "    \"{option_d_explanation}\\n\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# BACKGROUND INFORMATION OF YOUR ROLE:\n",
      "You are an expert in machine learning.\n",
      "You working (with some other professionals) verifying if a question from a bank of question is ok to be used in a job interview.\n",
      "The question has already been analized by other professionals. Their analysis is below.\n",
      "It was already pointed that this question could have more than one option as correct, but it shouldn't. It should have just one correct option.\n",
      "# YOUR TASK\n",
      "your task is to fix the questions and/or the options in a way that it mantain the overall ideia for this question, but it should have after your correction, just one option as correct.\n",
      "Also, you should give a detailed explanation for your answer.\n",
      "\n",
      "# ORIGINAL QUESTION\n",
      "\n",
      "Which term represents correctly classified positive cases?\n",
      "\n",
      "# Option a\n",
      "True Positives\n",
      "\n",
      "# Option b\n",
      "False Positives\n",
      "\n",
      "# Option c\n",
      "False Negatives\n",
      "\n",
      "# Option d\n",
      "True Negatives\n",
      "\n",
      "# PROFESSIONAL ONE ANALYSIS\n",
      "option_a_explanation\n",
      "True Positives (TP) are the results that indicate the presence of a condition (positive cases) correctly identified by a classification model. In binary classification, when we say a case is a true positive, it means that the model has correctly predicted the positive class, thus, this is the correct definition for correctly classified positive cases.\n",
      "option_b_explanation\n",
      "False Positives (FP) refer to the instances where the model predicts the positive class incorrectly; that is, the model predicted a positive when the actual case was negative. This does not represent correctly classified positive cases, hence this option is incorrect.\n",
      "option_c_explanation\n",
      "False Negatives (FN) are instances where the model fails to identify a positive case and predicts it as negative. This results in missing a positive case, and thus, is the opposite of correctly classified positive cases. Therefore, this option is also incorrect.\n",
      "option_d_explanation\n",
      "True Negatives (TN) represent instances where the model correctly predicts the negative class. While this is a correct classification, it does not pertain to positive cases—hence, this option does not represent correctly classified positive cases either.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_fix_question_formatted = prompt_fix_question.format(\n",
    "        question=question[\"question\"],\n",
    "        option_a=question[\"answer_a\"],\n",
    "        option_b=question[\"answer_b\"],\n",
    "        option_c=question[\"answer_c\"],\n",
    "        option_d=question[\"answer_d\"],\n",
    "        \n",
    "        option_a_explanation=analysis_openai[\"option_a_explanation\"],\n",
    "        option_b_explanation=analysis_openai[\"option_b_explanation\"],\n",
    "        option_c_explanation=analysis_openai[\"option_c_explanation\"],\n",
    "        option_d_explanation=analysis_openai[\"option_d_explanation\"],\n",
    "    )\n",
    "\n",
    "print( prompt_fix_question_formatted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_fix_question_formatted},\n",
    "        # {\"role\": \"user\", \"content\": \"hello\"}\n",
    "    ],\n",
    "    response_format=ExplanationSchema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExplanationSchema(option_a_explanation=\"Option a, 'True Positives', is correct because it specifically refers to the cases in which the positive instances are correctly identified by the model. In terms of classification outcomes, true positives are those instances that are actual positives, and the model has correctly classified them as positive.\", option_b_explanation=\"Option b, 'False Positives', is incorrect because it refers to the cases where the model incorrectly classifies negative instances as positive. This does not represent correctly classified positive cases.\", option_c_explanation=\"Option c, 'False Negatives', is also incorrect as it represents the cases where actual positive instances are incorrectly classified as negative. This means that the positive cases are not correctly identified by the model, which is the opposite of what the question requires.\", option_d_explanation=\"Option d, 'True Negatives', is incorrect because it refers to the instances that are correctly classified as negative. This does not relate to the classification of positive cases at all.\", just_one_option_correct=True, correct_answer='a')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question_start: dict\n",
    "    analysis_gemini: dict\n",
    "    analysis_llama: dict\n",
    "    analysis_openai: dict\n",
    "    question_openai_fixed: dict\n",
    "    second_time: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_analysis_gemini(state: State):\n",
    "    question = state[\"question_start\"]\n",
    "\n",
    "    prompt_revision_formatted = prompt_revision.format(\n",
    "        question=question[\"question\"],\n",
    "        option_a=question[\"answer_a\"],\n",
    "        option_b=question[\"answer_b\"],\n",
    "        option_c=question[\"answer_c\"],\n",
    "        option_d=question[\"answer_d\"],        \n",
    "    )\n",
    "\n",
    "    response = llm_gemini.generate_content(prompt_revision_formatted)\n",
    "    analysis_gemini = json_repair.loads(response.text)\n",
    "    \n",
    "    return { \"analysis_gemini\": analysis_gemini }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_analysis_llama(state: State):\n",
    "    question = state[\"question_start\"]\n",
    "\n",
    "    prompt_revision_with_json = prompt_revision + (\n",
    "        \"# OUTPUT FORMAT: json\\n\\n\"\n",
    "        \"{output_format}\"\n",
    "    )\n",
    "\n",
    "    prompt_revision_formatted = prompt_revision_with_json.format(\n",
    "        question=question[\"question\"],\n",
    "        option_a=question[\"answer_a\"],\n",
    "        option_b=question[\"answer_b\"],\n",
    "        option_c=question[\"answer_c\"],\n",
    "        option_d=question[\"answer_d\"],\n",
    "        output_format=json.dumps(ExplanationSchema.model_json_schema(), indent=2)\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        # model=\"llama3-8b-8192\",\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_revision_formatted\n",
    "        }],\n",
    "        temperature=0.5,\n",
    "        # max_tokens=2420,\n",
    "        top_p=0.95,\n",
    "        stream=False,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    completion = response.choices[0].message\n",
    "    analysis_llama = json_repair.loads(completion.content)\n",
    "    \n",
    "    return { \"analysis_llama\": analysis_llama }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_analysis_openai(state: State):\n",
    "    question = state[\"question_start\"]\n",
    "\n",
    "    prompt_revision_formatted = prompt_revision.format(\n",
    "        question=question[\"question\"],\n",
    "        option_a=question[\"answer_a\"],\n",
    "        option_b=question[\"answer_b\"],\n",
    "        option_c=question[\"answer_c\"],\n",
    "        option_d=question[\"answer_d\"],\n",
    "    )\n",
    "    \n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt_revision_formatted},\n",
    "            # {\"role\": \"user\", \"content\": \"hello\"}\n",
    "        ],\n",
    "        response_format=ExplanationSchema,\n",
    "    )\n",
    "\n",
    "    analysis_openai = json_repair.loads( completion.choices[0].message.parsed.model_dump_json() )\n",
    "\n",
    "    return { \"analysis_openai\": analysis_openai }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_try_fix_question(state: State):\n",
    "    question = state[\"question_start\"]\n",
    "\n",
    "    prompt_fix_question_formatted = prompt_fix_question.format(\n",
    "        question=question[\"question\"],\n",
    "        option_a=question[\"answer_a\"],\n",
    "        option_b=question[\"answer_b\"],\n",
    "        option_c=question[\"answer_c\"],\n",
    "        option_d=question[\"answer_d\"],\n",
    "        \n",
    "        option_a_explanation=analysis_openai[\"option_a_explanation\"],\n",
    "        option_b_explanation=analysis_openai[\"option_b_explanation\"],\n",
    "        option_c_explanation=analysis_openai[\"option_c_explanation\"],\n",
    "        option_d_explanation=analysis_openai[\"option_d_explanation\"],\n",
    "    )\n",
    "\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt_fix_question_formatted},\n",
    "            # {\"role\": \"user\", \"content\": \"hello\"}\n",
    "        ],\n",
    "        response_format=ExplanationSchema,\n",
    "    )\n",
    "\n",
    "    question_openai_fixed = json_repair.loads( completion.choices[0].message.parsed.model_dump_json() )\n",
    "\n",
    "    return { \"question_openai_fixed\": question_openai_fixed}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_correct_answer_is_equal(state: State):\n",
    "    analysis_gemini = state[\"analysis_gemini\"]\n",
    "    analysis_llama = state[\"analysis_llama\"]\n",
    "\n",
    "    if analysis_gemini[\"correct_answer\"] == analysis_llama['correct_answer']:\n",
    "        if analysis_gemini[\"just_one_option_correct\"] and analysis_llama['just_one_option_correct']:\n",
    "            return \"save_question\"\n",
    "    else:\n",
    "        return \"verify_trying_to_fix_second_time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7fa88b90dd10>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"chain_analysis_gemini\", chain_analysis_gemini)\n",
    "graph_builder.add_node(\"chain_analysis_llama\", chain_analysis_llama)\n",
    "graph_builder.add_node(\"chain_analysis_openai\", chain_analysis_openai)\n",
    "graph_builder.add_node(\"chain_try_fix_question\", chain_try_fix_question)\n",
    "\n",
    "graph_builder.add_edge(START, \"chain_analysis_gemini\")\n",
    "graph_builder.add_edge(\"chain_analysis_gemini\", \"chain_analysis_llama\")\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"chain_analysis_llama\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    verify_correct_answer_is_equal,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
