{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question revision chain\n",
    "\n",
    "- get a question\n",
    "- LLM A and B verify if the question is OK\n",
    "    - there is more than one option correct\n",
    "    - sugests modifications to improve the questions and options\n",
    "    - sugest a detailed explanation to give to the student\n",
    "- verify if \n",
    "    correct_answer is equal for both\n",
    "    AND\n",
    "    there is just one option true \n",
    "    - if YES: just take the explanations from gemini and set the correct answer\n",
    "    - if NO: \n",
    "        - if there is more than one option\n",
    "            - if YES:\n",
    "                - if it is not passing this part for the second time\n",
    "                    - if YES:\n",
    "                        - fix the options\n",
    "                        - go to the start of flow\n",
    "                    - if NO: \n",
    "                        - discart this question because it already tried to fix it\n",
    "            - if NO: \n",
    "                - take a vote from openai\n",
    "                - go to the part of save the question because it should have a majority vote for one option\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json_repair\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 953 entries, 0 to 952\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   id                 953 non-null    int64 \n",
      " 1   created_at         953 non-null    object\n",
      " 2   subject_matter     953 non-null    object\n",
      " 3   topic_description  953 non-null    object\n",
      " 4   level              953 non-null    object\n",
      " 5   question           953 non-null    object\n",
      " 6   type               953 non-null    object\n",
      " 7   answer_correct     953 non-null    object\n",
      " 8   explanation        953 non-null    object\n",
      " 9   answer_a           953 non-null    object\n",
      " 10  answer_b           953 non-null    object\n",
      " 11  answer_c           953 non-null    object\n",
      " 12  answer_d           953 non-null    object\n",
      "dtypes: int64(1), object(12)\n",
      "memory usage: 96.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"questions.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                  11\n",
       "created_at                               2024-09-22 12:31:43.546285+00\n",
       "subject_matter                                             Probability\n",
       "topic_description                       Understanding Confusion Matrix\n",
       "level                                                     1 - Remember\n",
       "question             Which term represents correctly classified pos...\n",
       "type                                                  multiple_options\n",
       "answer_correct                                                       a\n",
       "explanation          True Positives (TP) represent the number of ca...\n",
       "answer_a                                                True Positives\n",
       "answer_b                                               False Positives\n",
       "answer_c                                               False Negatives\n",
       "answer_d                                                True Negatives\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = df.iloc[0]\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which term represents correctly classified positive cases?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question[\"question\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/quiz_interview/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from google.ai.generativelanguage_v1beta.types import content\n",
    "\n",
    "key_gemini = os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "genai.configure(api_key=key_gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = content.Schema(\n",
    "    type=content.Type.OBJECT,\n",
    "    properties={\n",
    "        \"option_a_explanation\": content.Schema(\n",
    "            type=content.Type.STRING,\n",
    "            description=\"Detailed explanation of why the option 'a' is correct or not.\",\n",
    "        ),\n",
    "        \"option_b_explanation\": content.Schema(\n",
    "            type=content.Type.STRING,\n",
    "            description=\"Detailed explanation of why the option 'b' is correct or not.\",\n",
    "        ),\n",
    "        \"option_c_explanation\": content.Schema(\n",
    "            type=content.Type.STRING,\n",
    "            description=\"Detailed explanation of why the option 'c' is correct or not.\",\n",
    "        ),\n",
    "        \"option_d_explanation\": content.Schema(\n",
    "            type=content.Type.STRING,\n",
    "            description=\"Detailed explanation of why the option 'd' is correct or not.\",\n",
    "        ),\n",
    "        \"just_one_option_correct\": content.Schema(\n",
    "            type=content.Type.BOOLEAN,\n",
    "            description=\"If there is just one option correct.\",\n",
    "        ),\n",
    "        \"correct_answer\": content.Schema(\n",
    "            type=content.Type.STRING,\n",
    "            description=\"The correct option. It could be 'a', 'b', 'c', 'd'.\",\n",
    "        ),\n",
    "    },\n",
    "    required=[\n",
    "        \"option_a_explanation\",\n",
    "        \"option_b_explanation\",\n",
    "        \"option_c_explanation\",\n",
    "        \"option_d_explanation\",\n",
    "        \"just_one_option_correct\",\n",
    "        \"correct_answer\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "generation_config = {\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.95,\n",
    "    # \"top_k\": 64,\n",
    "    # \"max_output_tokens\": 8192,\n",
    "    \"response_schema\": schema,\n",
    "    \"response_mime_type\": \"application/json\",\n",
    "}\n",
    "    \n",
    "\n",
    "llm_gemini = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=generation_config,\n",
    "    # safety_settings = Adjust safety settings\n",
    "    # See https://ai.google.dev/gemini-api/docs/safety-settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_revision = (\n",
    "    \"# BACKGROUND INFORMATION OF YOUR ROLE:\\n\"\n",
    "    \"You are an expert in machine learning.\\n\"\n",
    "    \"You are verifying if a question from a bank of question is ok to be used in a job interview.\\n\"\n",
    "    \"your task is to analyse the provided question to verify if there is some problem.\\n\"\n",
    "    \"You shoud verify if there is just one option correct or if there is more than one.\\n\"\n",
    "    \"The question could be perfect fine, so you could say it too.\\n\"\n",
    "    \"Also, you should give a detailed explanation for your answer.\\n\"\n",
    "    \"# QUESTION\\n\\n\"\n",
    "    \"{question}\\n\\n\"\n",
    "    \"# Option a\\n\"\n",
    "    \"{option_a}\\n\\n\"\n",
    "    \"# Option b\\n\"\n",
    "    \"{option_b}\\n\\n\"\n",
    "    \"# Option c\\n\"\n",
    "    \"{option_c}\\n\\n\"\n",
    "    \"# Option d\\n\"\n",
    "    \"{option_d}\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# BACKGROUND INFORMATION OF YOUR ROLE:\n",
      "You are an expert in machine learning.\n",
      "You are verifying if a question from a bank of question is ok to be used in a job interview.\n",
      "your task is to analyse the provided question to verify if there is some problem.\n",
      "You shoud verify if there is just one option correct or if there is more than one.\n",
      "The question could be perfect fine, so you could say it too.\n",
      "Also, you should give a detailed explanation for your answer.\n",
      "# QUESTION\n",
      "\n",
      "Which term represents correctly classified positive cases?\n",
      "\n",
      "# Option a\n",
      "True Positives\n",
      "\n",
      "# Option b\n",
      "False Positives\n",
      "\n",
      "# Option c\n",
      "False Negatives\n",
      "\n",
      "# Option d\n",
      "True Negatives\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_revision_fromatted = prompt_revision.format(\n",
    "        question=question[\"question\"],\n",
    "        option_a=question[\"answer_a\"],\n",
    "        option_b=question[\"answer_b\"],\n",
    "        option_c=question[\"answer_c\"],\n",
    "        option_d=question[\"answer_d\"],        \n",
    "    )\n",
    "\n",
    "print( prompt_revision_fromatted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"{\\\"correct_answer\\\": \\\"a\\\", \\\"just_one_option_correct\\\": true, \\\"option_a_explanation\\\": \\\"True Positives (TP) represent the cases where the model correctly predicts a positive outcome when the actual outcome is also positive. It is a measure of how well the model identifies positive cases.\\\", \\\"option_b_explanation\\\": \\\"False Positives (FP) represent the cases where the model incorrectly predicts a positive outcome when the actual outcome is negative. It is a measure of how often the model incorrectly identifies a positive case.\\\", \\\"option_c_explanation\\\": \\\"False Negatives (FN) represent the cases where the model incorrectly predicts a negative outcome when the actual outcome is positive. It is a measure of how often the model fails to identify a positive case.\\\", \\\"option_d_explanation\\\": \\\"True Negatives (TN) represent the cases where the model correctly predicts a negative outcome when the actual outcome is also negative. It is a measure of how well the model identifies negative cases.\\\"} \"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 149,\n",
       "        \"candidates_token_count\": 207,\n",
       "        \"total_token_count\": 356\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm_gemini.generate_content(prompt_revision_fromatted)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correct_answer': 'a',\n",
       " 'just_one_option_correct': True,\n",
       " 'option_a_explanation': 'True Positives (TP) represent the cases where the model correctly predicts a positive outcome when the actual outcome is also positive. It is a measure of how well the model identifies positive cases.',\n",
       " 'option_b_explanation': 'False Positives (FP) represent the cases where the model incorrectly predicts a positive outcome when the actual outcome is negative. It is a measure of how often the model incorrectly identifies a positive case.',\n",
       " 'option_c_explanation': 'False Negatives (FN) represent the cases where the model incorrectly predicts a negative outcome when the actual outcome is positive. It is a measure of how often the model fails to identify a positive case.',\n",
       " 'option_d_explanation': 'True Negatives (TN) represent the cases where the model correctly predicts a negative outcome when the actual outcome is also negative. It is a measure of how well the model identifies negative cases.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemeni_analysis = json_repair.loads(response.text)\n",
    "gemeni_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplanationSchema(BaseModel):\n",
    "    option_a_explanation: str = Field(..., description=\"Detailed explanation of why the option 'a' is correct or not.\")\n",
    "    option_b_explanation: str = Field(..., description=\"Detailed explanation of why the option 'b' is correct or not.\")\n",
    "    option_c_explanation: str = Field(..., description=\"Detailed explanation of why the option 'c' is correct or not.\")\n",
    "    option_d_explanation: str = Field(..., description=\"Detailed explanation of why the option 'd' is correct or not.\")\n",
    "    just_one_option_correct: bool = Field(..., description=\"If there is just one option correct.\")\n",
    "    correct_answer: Literal['a', 'b', 'c', 'd'] = Field(..., description=\"The correct option. It could be 'a', 'b', 'c', or 'd'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "key_groq = os.environ[\"GROQ_API_KEY\"]\n",
    "client = Groq(api_key=key_groq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_revision_with_json = prompt_revision + (\n",
    "    \"# OUTPUT FORMAT: json\\n\\n\"\n",
    "    \"{output_format}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# BACKGROUND INFORMATION OF YOUR ROLE:\n",
      "You are an expert in machine learning.\n",
      "You are verifying if a question from a bank of question is ok to be used in a job interview.\n",
      "your task is to analyse the provided question to verify if there is some problem.\n",
      "You shoud verify if there is just one option correct or if there is more than one.\n",
      "The question could be perfect fine, so you could say it too.\n",
      "Also, you should give a detailed explanation for your answer.\n",
      "# QUESTION\n",
      "\n",
      "Which term represents correctly classified positive cases?\n",
      "\n",
      "# Option a\n",
      "True Positives\n",
      "\n",
      "# Option b\n",
      "False Positives\n",
      "\n",
      "# Option c\n",
      "False Negatives\n",
      "\n",
      "# Option d\n",
      "True Negatives\n",
      "\n",
      "# OUTPUT FORMAT: json\n",
      "\n",
      "{\n",
      "  \"properties\": {\n",
      "    \"option_a_explanation\": {\n",
      "      \"description\": \"Detailed explanation of why the option 'a' is correct or not.\",\n",
      "      \"title\": \"Option A Explanation\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"option_b_explanation\": {\n",
      "      \"description\": \"Detailed explanation of why the option 'b' is correct or not.\",\n",
      "      \"title\": \"Option B Explanation\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"option_c_explanation\": {\n",
      "      \"description\": \"Detailed explanation of why the option 'c' is correct or not.\",\n",
      "      \"title\": \"Option C Explanation\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"option_d_explanation\": {\n",
      "      \"description\": \"Detailed explanation of why the option 'd' is correct or not.\",\n",
      "      \"title\": \"Option D Explanation\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"just_one_option_correct\": {\n",
      "      \"description\": \"If there is just one option correct.\",\n",
      "      \"title\": \"Just One Option Correct\",\n",
      "      \"type\": \"boolean\"\n",
      "    },\n",
      "    \"correct_answer\": {\n",
      "      \"description\": \"The correct option. It could be 'a', 'b', 'c', or 'd'.\",\n",
      "      \"enum\": [\n",
      "        \"a\",\n",
      "        \"b\",\n",
      "        \"c\",\n",
      "        \"d\"\n",
      "      ],\n",
      "      \"title\": \"Correct Answer\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"option_a_explanation\",\n",
      "    \"option_b_explanation\",\n",
      "    \"option_c_explanation\",\n",
      "    \"option_d_explanation\",\n",
      "    \"just_one_option_correct\",\n",
      "    \"correct_answer\"\n",
      "  ],\n",
      "  \"title\": \"ExplanationSchema\",\n",
      "  \"type\": \"object\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt_revision_fromatted = prompt_revision_with_json.format(\n",
    "        question=question[\"question\"],\n",
    "        option_a=question[\"answer_a\"],\n",
    "        option_b=question[\"answer_b\"],\n",
    "        option_c=question[\"answer_c\"],\n",
    "        option_d=question[\"answer_d\"],\n",
    "        output_format=json.dumps(ExplanationSchema.model_json_schema(), indent=2)\n",
    "    )\n",
    "\n",
    "print( prompt_revision_fromatted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'option_a_explanation': 'This option is correct because True Positives represent the cases that are correctly classified as positive. In the context of machine learning, this term is used to describe the number of instances that are correctly predicted as belonging to the positive class.',\n",
       " 'option_b_explanation': 'This option is incorrect because False Positives represent the cases that are incorrectly classified as positive. In other words, these are the instances that do not belong to the positive class but are predicted as such.',\n",
       " 'option_c_explanation': 'This option is incorrect because False Negatives represent the cases that are incorrectly classified as negative. In other words, these are the instances that belong to the positive class but are predicted as not belonging to it.',\n",
       " 'option_d_explanation': 'This option is incorrect because True Negatives represent the cases that are correctly classified as negative. In other words, these are the instances that do not belong to the positive class and are correctly predicted as such.',\n",
       " 'just_one_option_correct': True,\n",
       " 'correct_answer': 'a'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    # model=\"llama3-8b-8192\",\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt_revision_fromatted\n",
    "    }],\n",
    "    temperature=0.5,\n",
    "    # max_tokens=2420,\n",
    "    top_p=0.95,\n",
    "    stream=False,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "completion = response.choices[0].message\n",
    "llama_analysis = json_repair.loads(completion.content)\n",
    "llama_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemeni_analysis[\"correct_answer\"] == llama_analysis['correct_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_merge = (\n",
    "    \"# BACKGROUND INFORMATION OF YOUR ROLE:\\n\"\n",
    "    \"You are an expert in machine learning.\\n\"\n",
    "    \"You working (with some other professionals) verifying if a question from a bank of question is ok to be used in a job interview.\\n\"\n",
    "    \"The question has already been analized by other professionals. Their analysis is below.\\n\"\n",
    "    \"# YOUR TASK\\n\"\n",
    "    \"your task is to give a detailed explanation for each option taken in consideration the analysis from the experts.\\n\"\n",
    "    \"You should also take in consideration the choosen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt_revision_fromatted},\n",
    "        # {\"role\": \"user\", \"content\": \"hello\"}\n",
    "    ],\n",
    "    response_format=ExplanationSchema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion.choices[0].message.parsed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
